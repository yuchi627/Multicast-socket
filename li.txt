"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.10346">arXiv:1903.10346</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.10346">pdf</a>, <a href="https://arxiv.org/format/1903.10346">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yao Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cottrell%2C+G">Garrison Cottrell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.10346v1-abstract-short" style="display: inline;">
        Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.10346v1-abstract-full').style.display = 'inline'; document.getElementById('1903.10346v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.10346v1-abstract-full" style="display: none;">
        Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples applied to speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes advances on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.10346v1-abstract-full').style.display = 'none'; document.getElementById('1903.10346v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.06293">arXiv:1903.06293</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.06293">pdf</a>, <a href="https://arxiv.org/ps/1903.06293">ps</a>, <a href="https://arxiv.org/format/1903.06293">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Research Agenda: Dynamic Models to Defend Against Correlated Attacks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.06293v1-abstract-short" style="display: inline;">
        In this article I describe a research agenda for securing machine learning models against adversarial inputs at test time. This article does not present results but instead shares some of my thoughts about where I think that the field needs to go. Modern machine learning works very well on I.I.D. data: data for which each example is drawn {\em independently} and for which the distribution generati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06293v1-abstract-full').style.display = 'inline'; document.getElementById('1903.06293v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.06293v1-abstract-full" style="display: none;">
        In this article I describe a research agenda for securing machine learning models against adversarial inputs at test time. This article does not present results but instead shares some of my thoughts about where I think that the field needs to go. Modern machine learning works very well on I.I.D. data: data for which each example is drawn {\em independently} and for which the distribution generating each example is {\em identical}. When these assumptions are relaxed, modern machine learning can perform very poorly. When machine learning is used in contexts where security is a concern, it is desirable to design models that perform well even when the input is designed by a malicious adversary. So far most research in this direction has focused on an adversary who violates the {\em identical} assumption, and imposes some kind of restricted worst-case distribution shift. I argue that machine learning security researchers should also address the problem of relaxing the {\em independence} assumption and that current strategies designed for robustness to distribution shift will not do so. I recommend {\em dynamic models} that change each time they are run as a potential solution path to this problem, and show an example of a simple attack using correlated data that can be mitigated by a simple dynamic defense. This is not intended as a real-world security measure, but as a recommendation to explore this research direction and develop more realistic defenses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06293v1-abstract-full').style.display = 'none'; document.getElementById('1903.06293v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.06705">arXiv:1902.06705</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.06705">pdf</a>, <a href="https://arxiv.org/ps/1902.06705">ps</a>, <a href="https://arxiv.org/format/1902.06705">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Evaluating Adversarial Robustness
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Athalye%2C+A">Anish Athalye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brendel%2C+W">Wieland Brendel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rauber%2C+J">Jonas Rauber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsipras%2C+D">Dimitris Tsipras</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Madry%2C+A">Aleksander Madry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.06705v2-abstract-short" style="display: inline;">
        Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.
  We believe a large contributing factor is the difficulty of performing security evaluations. In this pa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06705v2-abstract-full').style.display = 'inline'; document.getElementById('1902.06705v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.06705v2-abstract-full" style="display: none;">
        Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.
  We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06705v2-abstract-full').style.display = 'none'; document.getElementById('1902.06705v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 February, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Living document; source available at https://github.com/evaluating-adversarial-robustness/adv-eval-paper/</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.03685">arXiv:1811.03685</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.03685">pdf</a>, <a href="https://arxiv.org/format/1811.03685">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        New CleverHans Feature: Better Adversarial Robustness Evaluations with Attack Bundling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.03685v1-abstract-short" style="display: inline;">
        This technical report describes a new feature of the CleverHans library called "attack bundling". Many papers about adversarial examples present lists of error rates corresponding to different attack algorithms. A common approach is to take the maximum across this list and compare defenses against that error rate. We argue that a better approach is to use attack bundling: the max should be taken a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.03685v1-abstract-full').style.display = 'inline'; document.getElementById('1811.03685v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.03685v1-abstract-full" style="display: none;">
        This technical report describes a new feature of the CleverHans library called &#34;attack bundling&#34;. Many papers about adversarial examples present lists of error rates corresponding to different attack algorithms. A common approach is to take the maximum across this list and compare defenses against that error rate. We argue that a better approach is to use attack bundling: the max should be taken across many examples at the level of individual examples, then the error rate should be calculated by averaging after this maximization operation. Reporting the bundled attacker error rate provides a lower bound on the true worst-case error rate. The traditional approach of reporting the maximum error rate across attacks can underestimate the true worst-case error rate by an amount approaching 100\% as the number of attacks approaches infinity. Attack bundling can be used with different prioritization schemes to optimize quantities such as error rate on adversarial examples, perturbation size needed to cause misclassification, or failure rate when using a specific confidence threshold.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.03685v1-abstract-full').style.display = 'none'; document.getElementById('1811.03685v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.06758">arXiv:1810.06758</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.06758">pdf</a>, <a href="https://arxiv.org/format/1810.06758">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discriminator Rejection Sampling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Azadi%2C+S">Samaneh Azadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Darrell%2C+T">Trevor Darrell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.06758v3-abstract-short" style="display: inline;">
        We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be us&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.06758v3-abstract-full').style.display = 'inline'; document.getElementById('1810.06758v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.06758v3-abstract-full" style="display: none;">
        We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the SAGAN model, state-of-the-art in the image generation task at the time of developing this work. On ImageNet, we train an improved baseline that increases the Inception Score from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.06758v3-abstract-full').style.display = 'none'; document.getElementById('1810.06758v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2019</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03307">arXiv:1810.03307</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03307">pdf</a>, <a href="https://arxiv.org/format/1810.03307">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adebayo%2C+J">Julius Adebayo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Been Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03307v1-abstract-short" style="display: inline;">
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03307v1-abstract-full').style.display = 'inline'; document.getElementById('1810.03307v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03307v1-abstract-full" style="display: none;">
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN&#39;s output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN&#39;s architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et. al. (2018).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03307v1-abstract-full').style.display = 'none'; document.getElementById('1810.03307v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Workshop Track International Conference on Learning Representations (ICLR)</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03292">arXiv:1810.03292</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03292">pdf</a>, <a href="https://arxiv.org/format/1810.03292">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sanity Checks for Saliency Maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adebayo%2C+J">Julius Adebayo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Muelly%2C+M">Michael Muelly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hardt%2C+M">Moritz Hardt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Been Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03292v2-abstract-short" style="display: inline;">
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual ass&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03292v2-abstract-full').style.display = 'inline'; document.getElementById('1810.03292v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03292v2-abstract-full" style="display: none;">
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03292v2-abstract-full').style.display = 'none'; document.getElementById('1810.03292v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NIPS 2018 Camera Ready Version</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.08352">arXiv:1809.08352</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.08352">pdf</a>, <a href="https://arxiv.org/format/1809.08352">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unrestricted Adversarial Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chiyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Christiano%2C+P">Paul Christiano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.08352v1-abstract-short" style="display: inline;">
        We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08352v1-abstract-full').style.display = 'inline'; document.getElementById('1809.08352v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.08352v1-abstract-full" style="display: none;">
        We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. Attackers try to subvert defenses by finding arbitrary unambiguous inputs where the model assigns an incorrect label with high confidence. We propose a simple unambiguous dataset (&#34;bird-or- bicycle&#34;) to use as part of this contest. We hope this contest will help to more comprehensively evaluate the worst-case adversarial risk of machine learning models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08352v1-abstract-full').style.display = 'none'; document.getElementById('1809.08352v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.04888">arXiv:1808.04888</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.04888">pdf</a>, <a href="https://arxiv.org/format/1808.04888">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Skill Rating for Generative Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhupatiraju%2C+S">Surya Bhupatiraju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T">Tom Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.04888v1-abstract-short" style="display: inline;">
        We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different cont&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04888v1-abstract-full').style.display = 'inline'; document.getElementById('1808.04888v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.04888v1-abstract-full" style="display: none;">
        We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04888v1-abstract-full').style.display = 'none'; document.getElementById('1808.04888v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.10875">arXiv:1807.10875</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.10875">pdf</a>, <a href="https://arxiv.org/format/1807.10875">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.10875v1-abstract-short" style="display: inline;">
        Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.10875v1-abstract-full').style.display = 'inline'; document.getElementById('1807.10875v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.10875v1-abstract-full" style="display: none;">
        Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of CGF to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called TensorFuzz that implements the described techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.10875v1-abstract-full').style.display = 'none'; document.getElementById('1807.10875v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint - work in progress</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.07543">arXiv:1807.07543</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.07543">pdf</a>, <a href="https://arxiv.org/format/1807.07543">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Berthelot%2C+D">David Berthelot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roy%2C+A">Aurko Roy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.07543v2-abstract-short" style="display: inline;">
        Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can "interpolate": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.07543v2-abstract-full').style.display = 'inline'; document.getElementById('1807.07543v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.07543v2-abstract-full" style="display: none;">
        Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can &#34;interpolate&#34;: By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.07543v2-abstract-full').style.display = 'none'; document.getElementById('1807.07543v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.06732">arXiv:1807.06732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.06732">pdf</a>, <a href="https://arxiv.org/format/1807.06732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Motivating the Rules of the Game for Adversarial Example Research
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adams%2C+R+P">Ryan P. Adams</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andersen%2C+D">David Andersen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dahl%2C+G+E">George E. Dahl</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.06732v2-abstract-short" style="display: inline;">
        Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06732v2-abstract-full').style.display = 'inline'; document.getElementById('1807.06732v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.06732v2-abstract-full" style="display: none;">
        Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06732v2-abstract-full').style.display = 'none'; document.getElementById('1807.06732v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.11146">arXiv:1806.11146</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.11146">pdf</a>, <a href="https://arxiv.org/format/1806.11146">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Reprogramming of Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elsayed%2C+G+F">Gamaleldin F. Elsayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.11146v2-abstract-short" style="display: inline;">
        Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11146v2-abstract-full').style.display = 'inline'; document.getElementById('1806.11146v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.11146v2-abstract-full" style="display: none;">
        Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11146v2-abstract-full').style.display = 'none'; document.getElementById('1806.11146v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 November, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 June, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.04169">arXiv:1806.04169</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.04169">pdf</a>, <a href="https://arxiv.org/format/1806.04169">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Defense Against the Dark Arts: An overview of adversarial example security research and future research directions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.04169v1-abstract-short" style="display: inline;">
        This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.04169v1-abstract-full" style="display: none;">
        This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.04169v1-abstract-full').style.display = 'none'; document.getElementById('1806.04169v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.08318">arXiv:1805.08318</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.08318">pdf</a>, <a href="https://arxiv.org/format/1805.08318">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Attention Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Han Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metaxas%2C+D">Dimitris Metaxas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.08318v1-abstract-short" style="display: inline;">
        In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08318v1-abstract-full').style.display = 'inline'; document.getElementById('1805.08318v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.08318v1-abstract-full" style="display: none;">
        In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08318v1-abstract-full').style.display = 'none'; document.getElementById('1805.08318v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.07870">arXiv:1804.07870</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.07870">pdf</a>, <a href="https://arxiv.org/format/1804.07870">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.07870v1-abstract-short" style="display: inline;">
        A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.07870v1-abstract-full').style.display = 'inline'; document.getElementById('1804.07870v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.07870v1-abstract-full" style="display: none;">
        A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation that will fool the model, but security guarantees require a lower bound. CLEVER is a proposed scoring method to estimate a lower bound. Unfortunately, an estimate of a bound is not a bound. In this report, we show that gradient masking, a common problem that causes attack methodologies to provide only a very loose upper bound, causes CLEVER to overestimate the size of perturbation needed to fool the model. In other words, CLEVER does not resolve the key problem with the attack-based methodology, because it fails to provide a lower bound.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.07870v1-abstract-full').style.display = 'none'; document.getElementById('1804.07870v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.00097">arXiv:1804.00097</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.00097">pdf</a>, <a href="https://arxiv.org/format/1804.00097">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks and Defences Competition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yinpeng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+F">Fangzhou Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+M">Ming Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+T">Tianyu Pang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jun Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xiaolin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jianyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhishuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Z">Zhou Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuille%2C+A">Alan Yuille</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sangxia Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yuzhe Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhonglin Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+J">Junjiajia Long</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berdibekov%2C+Y">Yerkebulan Berdibekov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akiba%2C+T">Takuya Akiba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tokui%2C+S">Seiya Tokui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abe%2C+M">Motoki Abe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.00097v1-abstract-short" style="display: inline;">
        To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.00097v1-abstract-full').style.display = 'inline'; document.getElementById('1804.00097v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.00097v1-abstract-full" style="display: none;">
        To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.00097v1-abstract-full').style.display = 'none'; document.getElementById('1804.00097v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">36 pages, 10 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.06373">arXiv:1803.06373</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.06373">pdf</a>, <a href="https://arxiv.org/ps/1803.06373">ps</a>, <a href="https://arxiv.org/format/1803.06373">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Logit Pairing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kannan%2C+H">Harini Kannan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.06373v1-abstract-short" style="display: inline;">
        In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06373v1-abstract-full').style.display = 'inline'; document.getElementById('1803.06373v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.06373v1-abstract-full" style="display: none;">
        In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06373v1-abstract-full').style.display = 'none'; document.getElementById('1803.06373v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.08768">arXiv:1802.08768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.08768">pdf</a>, <a href="https://arxiv.org/format/1802.08768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Is Generator Conditioning Causally Related to GAN Performance?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buckman%2C+J">Jacob Buckman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olah%2C+C">Christopher Olah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.08768v2-abstract-short" style="display: inline;">
        Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08768v2-abstract-full').style.display = 'inline'; document.getElementById('1802.08768v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.08768v2-abstract-full" style="display: none;">
        Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (with z from p(z)) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the &#39;quality&#39; of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a &#39;regularization&#39; technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08768v2-abstract-full').style.display = 'none'; document.getElementById('1802.08768v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.08195">arXiv:1802.08195</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.08195">pdf</a>, <a href="https://arxiv.org/format/1802.08195">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Examples that Fool both Computer Vision and Time-Limited Humans
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elsayed%2C+G+F">Gamaleldin F. Elsayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shankar%2C+S">Shreya Shankar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+B">Brian Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alex Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.08195v3-abstract-short" style="display: inline;">
        Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08195v3-abstract-full').style.display = 'inline'; document.getElementById('1802.08195v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.08195v3-abstract-full" style="display: none;">
        Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08195v3-abstract-full').style.display = 'none'; document.getElementById('1802.08195v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.07736">arXiv:1801.07736</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.07736">pdf</a>, <a href="https://arxiv.org/format/1801.07736">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MaskGAN: Better Text Generation via Filling in the______
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fedus%2C+W">William Fedus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.07736v3-abstract-short" style="display: inline;">
        Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07736v3-abstract-full').style.display = 'inline'; document.getElementById('1801.07736v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.07736v3-abstract-full" style="display: none;">
        Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07736v3-abstract-full').style.display = 'none'; document.getElementById('1801.07736v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, ICLR 2018</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.02774">arXiv:1801.02774</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.02774">pdf</a>, <a href="https://arxiv.org/format/1801.02774">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Spheres
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metz%2C+L">Luke Metz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faghri%2C+F">Fartash Faghri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schoenholz%2C+S+S">Samuel S. Schoenholz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raghu%2C+M">Maithra Raghu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wattenberg%2C+M">Martin Wattenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.02774v3-abstract-short" style="display: inline;">
        State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.02774v3-abstract-full').style.display = 'inline'; document.getElementById('1801.02774v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.02774v3-abstract-full" style="display: none;">
        State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size $O(1/\sqrt{d})$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.02774v3-abstract-full').style.display = 'none'; document.getElementById('1801.02774v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T45
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1710.08446">arXiv:1710.08446</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1710.08446">pdf</a>, <a href="https://arxiv.org/format/1710.08446">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fedus%2C+W">William Fedus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosca%2C+M">Mihaela Rosca</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lakshminarayanan%2C+B">Balaji Lakshminarayanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+S">Shakir Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1710.08446v3-abstract-short" style="display: inline;">
        Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each playe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.08446v3-abstract-full').style.display = 'inline'; document.getElementById('1710.08446v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1710.08446v3-abstract-full" style="display: none;">
        Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players&#39; parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.08446v3-abstract-full').style.display = 'none'; document.getElementById('1710.08446v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 October, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1708.08022">arXiv:1708.08022</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1708.08022">pdf</a>, <a href="https://arxiv.org/ps/1708.08022">ps</a>, <a href="https://arxiv.org/format/1708.08022">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/CSF.2017.10">10.1109/CSF.2017.10 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Protection of Private Information in Machine Learning Systems: Two Recent Approaches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erlingsson%2C+%C3%9A">lfar Erlingsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mironov%2C+I">Ilya Mironov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1708.08022v1-abstract-short" style="display: inline;">
        The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08022v1-abstract-full').style.display = 'inline'; document.getElementById('1708.08022v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1708.08022v1-abstract-full" style="display: none;">
        The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled by Saltzer and Schroeder in the 1970s.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08022v1-abstract-full').style.display = 'none'; document.getElementById('1708.08022v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2017.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE 30th Computer Security Foundations Symposium (CSF), pages 1--6, 2017
      </p>
    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1705.07204">arXiv:1705.07204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1705.07204">pdf</a>, <a href="https://arxiv.org/format/1705.07204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ensemble Adversarial Training: Attacks and Defenses
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tram%C3%A8r%2C+F">Florian Tramr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boneh%2C+D">Dan Boneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1705.07204v4-abstract-short" style="display: inline;">
        Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07204v4-abstract-full').style.display = 'inline'; document.getElementById('1705.07204v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1705.07204v4-abstract-full" style="display: none;">
        Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model&#39;s loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07204v4-abstract-full').style.display = 'none'; document.getElementById('1705.07204v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 May, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 5 figures, International Conference on Learning Representations (ICLR) 2018</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1704.03453">arXiv:1704.03453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1704.03453">pdf</a>, <a href="https://arxiv.org/format/1704.03453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Space of Transferable Adversarial Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tram%C3%A8r%2C+F">Florian Tramr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boneh%2C+D">Dan Boneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1704.03453v2-abstract-short" style="display: inline;">
        Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.03453v2-abstract-full').style.display = 'inline'; document.getElementById('1704.03453v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1704.03453v2-abstract-full" style="display: none;">
        Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability.
  In the first quantitative analysis of the similarity of different models&#39; decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.03453v2-abstract-full').style.display = 'none'; document.getElementById('1704.03453v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 7 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1702.02284">arXiv:1702.02284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1702.02284">pdf</a>, <a href="https://arxiv.org/format/1702.02284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks on Neural Network Policies
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sandy Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+Y">Yan Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abbeel%2C+P">Pieter Abbeel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1702.02284v1-abstract-short" style="display: inline;">
        Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adver&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02284v1-abstract-full').style.display = 'inline'; document.getElementById('1702.02284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1702.02284v1-abstract-full" style="display: none;">
        Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02284v1-abstract-full').style.display = 'none'; document.getElementById('1702.02284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2017.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1701.00160">arXiv:1701.00160</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1701.00160">pdf</a>, <a href="https://arxiv.org/format/1701.00160">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NIPS 2016 Tutorial: Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1701.00160v4-abstract-short" style="display: inline;">
        This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs wi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00160v4-abstract-full').style.display = 'inline'; document.getElementById('1701.00160v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1701.00160v4-abstract-full" style="display: none;">
        This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00160v4-abstract-full').style.display = 'none'; document.getElementById('1701.00160v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 December, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">v2-v4 are all typo fixes. No substantive changes relative to v1</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1611.01236">arXiv:1611.01236</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1611.01236">pdf</a>, <a href="https://arxiv.org/format/1611.01236">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Machine Learning at Scale
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1611.01236v2-abstract-short" style="display: inline;">
        Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01236v2-abstract-full').style.display = 'inline'; document.getElementById('1611.01236v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1611.01236v2-abstract-full" style="display: none;">
        Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model&#39;s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a &#34;label leaking&#34; effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01236v2-abstract-full').style.display = 'none'; document.getElementById('1611.01236v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 November, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 5 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1610.05755">arXiv:1610.05755</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1610.05755">pdf</a>, <a href="https://arxiv.org/format/1610.05755">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erlingsson%2C+%C3%9A">lfar Erlingsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1610.05755v4-abstract-short" style="display: inline;">
        Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.05755v4-abstract-full').style.display = 'inline'; document.getElementById('1610.05755v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1610.05755v4-abstract-full" style="display: none;">
        Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as &#34;teachers&#34; for a &#34;student&#34; model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student&#39;s privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student&#39;s training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings.
  Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.05755v4-abstract-full').style.display = 'none'; document.getElementById('1610.05755v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 October, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICLR 17 as an oral</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1610.00768">arXiv:1610.00768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1610.00768">pdf</a>, <a href="https://arxiv.org/ps/1610.00768">ps</a>, <a href="https://arxiv.org/format/1610.00768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Technical Report on the CleverHans v2.1.0 Adversarial Examples Library
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faghri%2C+F">Fartash Faghri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feinman%2C+R">Reuben Feinman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+Y">Yash Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T">Tom Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roy%2C+A">Aurko Roy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matyasko%2C+A">Alexander Matyasko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behzadan%2C+V">Vahid Behzadan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hambardzumyan%2C+K">Karen Hambardzumyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhishuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Juang%2C+Y">Yi-Lin Juang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sheatsley%2C+R">Ryan Sheatsley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garg%2C+A">Abhibhav Garg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Uesato%2C+J">Jonathan Uesato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gierke%2C+W">Willi Gierke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yinpeng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berthelot%2C+D">David Berthelot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hendricks%2C+P">Paul Hendricks</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rauber%2C+J">Jonas Rauber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+R">Rujun Long</a>
      , et al. (1 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1610.00768v6-abstract-short" style="display: inline;">
        CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial exam&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.00768v6-abstract-full').style.display = 'inline'; document.getElementById('1610.00768v6-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1610.00768v6-abstract-full" style="display: none;">
        CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models&#39; performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure.
  This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the CleverHans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.00768v6-abstract-full').style.display = 'none'; document.getElementById('1610.00768v6-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 October, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical report for https://github.com/tensorflow/cleverhans</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.02533">arXiv:1607.02533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.02533">pdf</a>, <a href="https://arxiv.org/format/1607.02533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial examples in the physical world
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.02533v4-abstract-short" style="display: inline;">
        Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.02533v4-abstract-full').style.display = 'inline'; document.getElementById('1607.02533v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.02533v4-abstract-full" style="display: none;">
        Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.02533v4-abstract-full').style.display = 'none'; document.getElementById('1607.02533v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.00133">arXiv:1607.00133</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.00133">pdf</a>, <a href="https://arxiv.org/format/1607.00133">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/2976749.2978318">10.1145/2976749.2978318 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning with Differential Privacy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+A">Andy Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mironov%2C+I">Ilya Mironov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.00133v2-abstract-short" style="display: inline;">
        Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00133v2-abstract-full').style.display = 'inline'; document.getElementById('1607.00133v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.00133v2-abstract-full" style="display: none;">
        Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00133v2-abstract-full').style.display = 'none'; document.getElementById('1607.00133v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 October, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (ACM CCS), pp. 308-318, 2016
      </p>
    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1606.03498">arXiv:1606.03498</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1606.03498">pdf</a>, <a href="https://arxiv.org/format/1606.03498">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improved Techniques for Training GANs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Salimans%2C+T">Tim Salimans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaremba%2C+W">Wojciech Zaremba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+V">Vicki Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Radford%2C+A">Alec Radford</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xi Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1606.03498v1-abstract-short" style="display: inline;">
        We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, n&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.03498v1-abstract-full').style.display = 'inline'; document.getElementById('1606.03498v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1606.03498v1-abstract-full" style="display: none;">
        We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.03498v1-abstract-full').style.display = 'none'; document.getElementById('1606.03498v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 June, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2016.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07725">arXiv:1605.07725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07725">pdf</a>, <a href="https://arxiv.org/ps/1605.07725">ps</a>, <a href="https://arxiv.org/format/1605.07725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Training Methods for Semi-Supervised Text Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Miyato%2C+T">Takeru Miyato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07725v3-abstract-short" style="display: inline;">
        Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07725v3-abstract-full').style.display = 'inline'; document.getElementById('1605.07725v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07725v3-abstract-full" style="display: none;">
        Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07725v3-abstract-full').style.display = 'none'; document.getElementById('1605.07725v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 May, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2017</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07277">arXiv:1605.07277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07277">pdf</a>, <a href="https://arxiv.org/format/1605.07277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07277v1-abstract-short" style="display: inline;">
        Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07277v1-abstract-full').style.display = 'inline'; document.getElementById('1605.07277v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07277v1-abstract-full" style="display: none;">
        Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07277v1-abstract-full').style.display = 'none'; document.getElementById('1605.07277v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07157">arXiv:1605.07157</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07157">pdf</a>, <a href="https://arxiv.org/format/1605.07157">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Learning for Physical Interaction through Video Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Finn%2C+C">Chelsea Finn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levine%2C+S">Sergey Levine</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07157v4-abstract-short" style="display: inline;">
        A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about ph&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07157v4-abstract-full').style.display = 'inline'; document.getElementById('1605.07157v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07157v4-abstract-full" style="display: none;">
        A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot&#39;s future actions amounts to learning a &#34;visual imagination&#34; of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07157v4-abstract-full').style.display = 'none'; document.getElementById('1605.07157v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 October, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 May, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in NIPS &#39;16; Video results, code, and data available at: http://www.sites.google.com/site/robotprediction</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1604.04326">arXiv:1604.04326</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1604.04326">pdf</a>, <a href="https://arxiv.org/format/1604.04326">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving the Robustness of Deep Neural Networks via Stability Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+S">Stephan Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leung%2C+T">Thomas Leung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1604.04326v1-abstract-short" style="display: inline;">
        In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep network&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.04326v1-abstract-full').style.display = 'inline'; document.getElementById('1604.04326v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1604.04326v1-abstract-full" style="display: none;">
        In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.04326v1-abstract-full').style.display = 'none'; document.getElementById('1604.04326v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in CVPR 2016</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1603.04467">arXiv:1603.04467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1603.04467">pdf</a>, <a href="https://arxiv.org/format/1603.04467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+A">Ashish Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barham%2C+P">Paul Barham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brevdo%2C+E">Eugene Brevdo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhifeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Citro%2C+C">Craig Citro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Corrado%2C+G+S">Greg S. Corrado</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Davis%2C+A">Andy Davis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dean%2C+J">Jeffrey Dean</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Devin%2C+M">Matthieu Devin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghemawat%2C+S">Sanjay Ghemawat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harp%2C+A">Andrew Harp</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Irving%2C+G">Geoffrey Irving</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Isard%2C+M">Michael Isard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+Y">Yangqing Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jozefowicz%2C+R">Rafal Jozefowicz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaiser%2C+L">Lukasz Kaiser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kudlur%2C+M">Manjunath Kudlur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levenberg%2C+J">Josh Levenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mane%2C+D">Dan Mane</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Monga%2C+R">Rajat Monga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moore%2C+S">Sherry Moore</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Murray%2C+D">Derek Murray</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olah%2C+C">Chris Olah</a>
      , et al. (15 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1603.04467v2-abstract-short" style="display: inline;">
        TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1603.04467v2-abstract-full').style.display = 'inline'; document.getElementById('1603.04467v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1603.04467v2-abstract-full" style="display: none;">
        TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1603.04467v2-abstract-full').style.display = 'none'; document.getElementById('1603.04467v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 March, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Version 2 updates only the metadata, to correct the formatting of Martn Abadi&#39;s name</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1602.02697">arXiv:1602.02697</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1602.02697">pdf</a>, <a href="https://arxiv.org/format/1602.02697">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Practical Black-Box Attacks against Machine Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jha%2C+S">Somesh Jha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Celik%2C+Z+B">Z. Berkay Celik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Swami%2C+A">Ananthram Swami</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1602.02697v4-abstract-short" style="display: inline;">
        Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.02697v4-abstract-full').style.display = 'inline'; document.getElementById('1602.02697v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1602.02697v4-abstract-full" style="display: none;">
        Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.02697v4-abstract-full').style.display = 'none'; document.getElementById('1602.02697v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 February, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.05644">arXiv:1511.05644</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.05644">pdf</a>, <a href="https://arxiv.org/format/1511.05644">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Autoencoders
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Makhzani%2C+A">Alireza Makhzani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaitly%2C+N">Navdeep Jaitly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frey%2C+B">Brendan Frey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.05644v2-abstract-short" style="display: inline;">
        In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05644v2-abstract-full').style.display = 'inline'; document.getElementById('1511.05644v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.05644v2-abstract-full" style="display: none;">
        In this paper, we propose the &#34;adversarial autoencoder&#34; (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05644v2-abstract-full').style.display = 'none'; document.getElementById('1511.05644v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.05641">arXiv:1511.05641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.05641">pdf</a>, <a href="https://arxiv.org/format/1511.05641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Net2Net: Accelerating Learning via Knowledge Transfer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tianqi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.05641v4-abstract-short" style="display: inline;">
        We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05641v4-abstract-full').style.display = 'inline'; document.getElementById('1511.05641v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.05641v4-abstract-full" style="display: none;">
        We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05641v4-abstract-full').style.display = 'none'; document.getElementById('1511.05641v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2016 submission</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1510.01799">arXiv:1510.01799</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1510.01799">pdf</a>, <a href="https://arxiv.org/ps/1510.01799">ps</a>, <a href="https://arxiv.org/format/1510.01799">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Per-Example Gradient Computations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1510.01799v2-abstract-short" style="display: inline;">
        This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1510.01799v2-abstract-full" style="display: none;">
        This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1510.01799v2-abstract-full').style.display = 'none'; document.getElementById('1510.01799v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2015; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 October, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This revision fixed some typos. Many thanks to Hugo Larochelle for reporting them!</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1312.6199">arXiv:1312.6199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1312.6199">pdf</a>, <a href="https://arxiv.org/format/1312.6199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intriguing properties of neural networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Szegedy%2C+C">Christian Szegedy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaremba%2C+W">Wojciech Zaremba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sutskever%2C+I">Ilya Sutskever</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bruna%2C+J">Joan Bruna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erhan%2C+D">Dumitru Erhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fergus%2C+R">Rob Fergus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1312.6199v4-abstract-short" style="display: inline;">
        Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction betwee&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1312.6199v4-abstract-full').style.display = 'inline'; document.getElementById('1312.6199v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1312.6199v4-abstract-full" style="display: none;">
        Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.
  Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network&#39;s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1312.6199v4-abstract-full').style.display = 'none'; document.getElementById('1312.6199v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2014; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 December, 2013;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2013.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1212.2686">arXiv:1212.2686</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1212.2686">pdf</a>, <a href="https://arxiv.org/ps/1212.2686">ps</a>, <a href="https://arxiv.org/format/1212.2686">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Training of Deep Boltzmann Machines
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1212.2686v1-abstract-short" style="display: inline;">
        We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1212.2686v1-abstract-full" style="display: none;">
        We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1212.2686v1-abstract-full').style.display = 'none'; document.getElementById('1212.2686v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 December, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1211.5590">arXiv:1211.5590</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1211.5590">pdf</a>, <a href="https://arxiv.org/format/1211.5590">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Symbolic Computation">cs.SC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Theano: new features and speed improvements
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bastien%2C+F">Frdric Bastien</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lamblin%2C+P">Pascal Lamblin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pascanu%2C+R">Razvan Pascanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergstra%2C+J">James Bergstra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergeron%2C+A">Arnaud Bergeron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouchard%2C+N">Nicolas Bouchard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Warde-Farley%2C+D">David Warde-Farley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1211.5590v1-abstract-short" style="display: inline;">
        Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1211.5590v1-abstract-full').style.display = 'inline'; document.getElementById('1211.5590v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1211.5590v1-abstract-full" style="display: none;">
        Theano is a linear algebra compiler that optimizes a user&#39;s symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano&#39;s performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1211.5590v1-abstract-full').style.display = 'none'; document.getElementById('1211.5590v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at the Deep Learning Workshop, NIPS 2012</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1206.6407">arXiv:1206.6407</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1206.6407">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large-Scale Feature Learning With Spike-and-Slab Sparse Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1206.6407v1-abstract-short" style="display: inline;">
        We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enorm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1206.6407v1-abstract-full').style.display = 'inline'; document.getElementById('1206.6407v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1206.6407v1-abstract-full" style="display: none;">
        We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models? Transfer Learning Challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1206.6407v1-abstract-full').style.display = 'none'; document.getElementById('1206.6407v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap with arXiv:1201.3382</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.10346">arXiv:1903.10346</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.10346">pdf</a>, <a href="https://arxiv.org/format/1903.10346">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yao Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cottrell%2C+G">Garrison Cottrell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.10346v1-abstract-short" style="display: inline;">
        Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.10346v1-abstract-full').style.display = 'inline'; document.getElementById('1903.10346v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.10346v1-abstract-full" style="display: none;">
        Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples applied to speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes advances on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.10346v1-abstract-full').style.display = 'none'; document.getElementById('1903.10346v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.06293">arXiv:1903.06293</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.06293">pdf</a>, <a href="https://arxiv.org/ps/1903.06293">ps</a>, <a href="https://arxiv.org/format/1903.06293">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Research Agenda: Dynamic Models to Defend Against Correlated Attacks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.06293v1-abstract-short" style="display: inline;">
        In this article I describe a research agenda for securing machine learning models against adversarial inputs at test time. This article does not present results but instead shares some of my thoughts about where I think that the field needs to go. Modern machine learning works very well on I.I.D. data: data for which each example is drawn {\em independently} and for which the distribution generati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06293v1-abstract-full').style.display = 'inline'; document.getElementById('1903.06293v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.06293v1-abstract-full" style="display: none;">
        In this article I describe a research agenda for securing machine learning models against adversarial inputs at test time. This article does not present results but instead shares some of my thoughts about where I think that the field needs to go. Modern machine learning works very well on I.I.D. data: data for which each example is drawn {\em independently} and for which the distribution generating each example is {\em identical}. When these assumptions are relaxed, modern machine learning can perform very poorly. When machine learning is used in contexts where security is a concern, it is desirable to design models that perform well even when the input is designed by a malicious adversary. So far most research in this direction has focused on an adversary who violates the {\em identical} assumption, and imposes some kind of restricted worst-case distribution shift. I argue that machine learning security researchers should also address the problem of relaxing the {\em independence} assumption and that current strategies designed for robustness to distribution shift will not do so. I recommend {\em dynamic models} that change each time they are run as a potential solution path to this problem, and show an example of a simple attack using correlated data that can be mitigated by a simple dynamic defense. This is not intended as a real-world security measure, but as a recommendation to explore this research direction and develop more realistic defenses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06293v1-abstract-full').style.display = 'none'; document.getElementById('1903.06293v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.06705">arXiv:1902.06705</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.06705">pdf</a>, <a href="https://arxiv.org/ps/1902.06705">ps</a>, <a href="https://arxiv.org/format/1902.06705">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Evaluating Adversarial Robustness
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Athalye%2C+A">Anish Athalye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brendel%2C+W">Wieland Brendel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rauber%2C+J">Jonas Rauber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsipras%2C+D">Dimitris Tsipras</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Madry%2C+A">Aleksander Madry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.06705v2-abstract-short" style="display: inline;">
        Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.
  We believe a large contributing factor is the difficulty of performing security evaluations. In this pa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06705v2-abstract-full').style.display = 'inline'; document.getElementById('1902.06705v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.06705v2-abstract-full" style="display: none;">
        Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.
  We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06705v2-abstract-full').style.display = 'none'; document.getElementById('1902.06705v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 February, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Living document; source available at https://github.com/evaluating-adversarial-robustness/adv-eval-paper/</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.03685">arXiv:1811.03685</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.03685">pdf</a>, <a href="https://arxiv.org/format/1811.03685">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        New CleverHans Feature: Better Adversarial Robustness Evaluations with Attack Bundling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.03685v1-abstract-short" style="display: inline;">
        This technical report describes a new feature of the CleverHans library called "attack bundling". Many papers about adversarial examples present lists of error rates corresponding to different attack algorithms. A common approach is to take the maximum across this list and compare defenses against that error rate. We argue that a better approach is to use attack bundling: the max should be taken a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.03685v1-abstract-full').style.display = 'inline'; document.getElementById('1811.03685v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.03685v1-abstract-full" style="display: none;">
        This technical report describes a new feature of the CleverHans library called &#34;attack bundling&#34;. Many papers about adversarial examples present lists of error rates corresponding to different attack algorithms. A common approach is to take the maximum across this list and compare defenses against that error rate. We argue that a better approach is to use attack bundling: the max should be taken across many examples at the level of individual examples, then the error rate should be calculated by averaging after this maximization operation. Reporting the bundled attacker error rate provides a lower bound on the true worst-case error rate. The traditional approach of reporting the maximum error rate across attacks can underestimate the true worst-case error rate by an amount approaching 100\% as the number of attacks approaches infinity. Attack bundling can be used with different prioritization schemes to optimize quantities such as error rate on adversarial examples, perturbation size needed to cause misclassification, or failure rate when using a specific confidence threshold.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.03685v1-abstract-full').style.display = 'none'; document.getElementById('1811.03685v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.06758">arXiv:1810.06758</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.06758">pdf</a>, <a href="https://arxiv.org/format/1810.06758">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discriminator Rejection Sampling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Azadi%2C+S">Samaneh Azadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Darrell%2C+T">Trevor Darrell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.06758v3-abstract-short" style="display: inline;">
        We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be us&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.06758v3-abstract-full').style.display = 'inline'; document.getElementById('1810.06758v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.06758v3-abstract-full" style="display: none;">
        We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the SAGAN model, state-of-the-art in the image generation task at the time of developing this work. On ImageNet, we train an improved baseline that increases the Inception Score from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.06758v3-abstract-full').style.display = 'none'; document.getElementById('1810.06758v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2019</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03307">arXiv:1810.03307</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03307">pdf</a>, <a href="https://arxiv.org/format/1810.03307">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adebayo%2C+J">Julius Adebayo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Been Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03307v1-abstract-short" style="display: inline;">
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03307v1-abstract-full').style.display = 'inline'; document.getElementById('1810.03307v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03307v1-abstract-full" style="display: none;">
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN&#39;s output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN&#39;s architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et. al. (2018).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03307v1-abstract-full').style.display = 'none'; document.getElementById('1810.03307v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Workshop Track International Conference on Learning Representations (ICLR)</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03292">arXiv:1810.03292</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03292">pdf</a>, <a href="https://arxiv.org/format/1810.03292">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sanity Checks for Saliency Maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adebayo%2C+J">Julius Adebayo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Muelly%2C+M">Michael Muelly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hardt%2C+M">Moritz Hardt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Been Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03292v2-abstract-short" style="display: inline;">
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual ass&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03292v2-abstract-full').style.display = 'inline'; document.getElementById('1810.03292v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03292v2-abstract-full" style="display: none;">
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03292v2-abstract-full').style.display = 'none'; document.getElementById('1810.03292v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NIPS 2018 Camera Ready Version</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.08352">arXiv:1809.08352</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.08352">pdf</a>, <a href="https://arxiv.org/format/1809.08352">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unrestricted Adversarial Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chiyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Christiano%2C+P">Paul Christiano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.08352v1-abstract-short" style="display: inline;">
        We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08352v1-abstract-full').style.display = 'inline'; document.getElementById('1809.08352v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.08352v1-abstract-full" style="display: none;">
        We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. Attackers try to subvert defenses by finding arbitrary unambiguous inputs where the model assigns an incorrect label with high confidence. We propose a simple unambiguous dataset (&#34;bird-or- bicycle&#34;) to use as part of this contest. We hope this contest will help to more comprehensively evaluate the worst-case adversarial risk of machine learning models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08352v1-abstract-full').style.display = 'none'; document.getElementById('1809.08352v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.04888">arXiv:1808.04888</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.04888">pdf</a>, <a href="https://arxiv.org/format/1808.04888">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Skill Rating for Generative Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhupatiraju%2C+S">Surya Bhupatiraju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T">Tom Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.04888v1-abstract-short" style="display: inline;">
        We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different cont&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04888v1-abstract-full').style.display = 'inline'; document.getElementById('1808.04888v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.04888v1-abstract-full" style="display: none;">
        We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04888v1-abstract-full').style.display = 'none'; document.getElementById('1808.04888v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.10875">arXiv:1807.10875</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.10875">pdf</a>, <a href="https://arxiv.org/format/1807.10875">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.10875v1-abstract-short" style="display: inline;">
        Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.10875v1-abstract-full').style.display = 'inline'; document.getElementById('1807.10875v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.10875v1-abstract-full" style="display: none;">
        Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of CGF to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called TensorFuzz that implements the described techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.10875v1-abstract-full').style.display = 'none'; document.getElementById('1807.10875v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint - work in progress</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.07543">arXiv:1807.07543</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.07543">pdf</a>, <a href="https://arxiv.org/format/1807.07543">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Berthelot%2C+D">David Berthelot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roy%2C+A">Aurko Roy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.07543v2-abstract-short" style="display: inline;">
        Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can "interpolate": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.07543v2-abstract-full').style.display = 'inline'; document.getElementById('1807.07543v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.07543v2-abstract-full" style="display: none;">
        Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can &#34;interpolate&#34;: By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.07543v2-abstract-full').style.display = 'none'; document.getElementById('1807.07543v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.06732">arXiv:1807.06732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.06732">pdf</a>, <a href="https://arxiv.org/format/1807.06732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Motivating the Rules of the Game for Adversarial Example Research
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adams%2C+R+P">Ryan P. Adams</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andersen%2C+D">David Andersen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dahl%2C+G+E">George E. Dahl</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.06732v2-abstract-short" style="display: inline;">
        Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06732v2-abstract-full').style.display = 'inline'; document.getElementById('1807.06732v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.06732v2-abstract-full" style="display: none;">
        Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06732v2-abstract-full').style.display = 'none'; document.getElementById('1807.06732v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.11146">arXiv:1806.11146</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.11146">pdf</a>, <a href="https://arxiv.org/format/1806.11146">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Reprogramming of Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elsayed%2C+G+F">Gamaleldin F. Elsayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.11146v2-abstract-short" style="display: inline;">
        Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11146v2-abstract-full').style.display = 'inline'; document.getElementById('1806.11146v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.11146v2-abstract-full" style="display: none;">
        Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11146v2-abstract-full').style.display = 'none'; document.getElementById('1806.11146v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 November, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 June, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.04169">arXiv:1806.04169</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.04169">pdf</a>, <a href="https://arxiv.org/format/1806.04169">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Defense Against the Dark Arts: An overview of adversarial example security research and future research directions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.04169v1-abstract-short" style="display: inline;">
        This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.04169v1-abstract-full" style="display: none;">
        This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.04169v1-abstract-full').style.display = 'none'; document.getElementById('1806.04169v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.08318">arXiv:1805.08318</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.08318">pdf</a>, <a href="https://arxiv.org/format/1805.08318">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Attention Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Han Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metaxas%2C+D">Dimitris Metaxas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.08318v1-abstract-short" style="display: inline;">
        In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08318v1-abstract-full').style.display = 'inline'; document.getElementById('1805.08318v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.08318v1-abstract-full" style="display: none;">
        In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08318v1-abstract-full').style.display = 'none'; document.getElementById('1805.08318v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.07870">arXiv:1804.07870</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.07870">pdf</a>, <a href="https://arxiv.org/format/1804.07870">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.07870v1-abstract-short" style="display: inline;">
        A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.07870v1-abstract-full').style.display = 'inline'; document.getElementById('1804.07870v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.07870v1-abstract-full" style="display: none;">
        A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation that will fool the model, but security guarantees require a lower bound. CLEVER is a proposed scoring method to estimate a lower bound. Unfortunately, an estimate of a bound is not a bound. In this report, we show that gradient masking, a common problem that causes attack methodologies to provide only a very loose upper bound, causes CLEVER to overestimate the size of perturbation needed to fool the model. In other words, CLEVER does not resolve the key problem with the attack-based methodology, because it fails to provide a lower bound.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.07870v1-abstract-full').style.display = 'none'; document.getElementById('1804.07870v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.00097">arXiv:1804.00097</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.00097">pdf</a>, <a href="https://arxiv.org/format/1804.00097">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks and Defences Competition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yinpeng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+F">Fangzhou Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+M">Ming Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+T">Tianyu Pang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jun Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xiaolin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jianyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhishuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Z">Zhou Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuille%2C+A">Alan Yuille</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sangxia Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yuzhe Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhonglin Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+J">Junjiajia Long</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berdibekov%2C+Y">Yerkebulan Berdibekov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akiba%2C+T">Takuya Akiba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tokui%2C+S">Seiya Tokui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abe%2C+M">Motoki Abe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.00097v1-abstract-short" style="display: inline;">
        To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.00097v1-abstract-full').style.display = 'inline'; document.getElementById('1804.00097v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.00097v1-abstract-full" style="display: none;">
        To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.00097v1-abstract-full').style.display = 'none'; document.getElementById('1804.00097v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">36 pages, 10 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.06373">arXiv:1803.06373</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.06373">pdf</a>, <a href="https://arxiv.org/ps/1803.06373">ps</a>, <a href="https://arxiv.org/format/1803.06373">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Logit Pairing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kannan%2C+H">Harini Kannan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.06373v1-abstract-short" style="display: inline;">
        In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06373v1-abstract-full').style.display = 'inline'; document.getElementById('1803.06373v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.06373v1-abstract-full" style="display: none;">
        In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06373v1-abstract-full').style.display = 'none'; document.getElementById('1803.06373v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.08768">arXiv:1802.08768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.08768">pdf</a>, <a href="https://arxiv.org/format/1802.08768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Is Generator Conditioning Causally Related to GAN Performance?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buckman%2C+J">Jacob Buckman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olah%2C+C">Christopher Olah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.08768v2-abstract-short" style="display: inline;">
        Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08768v2-abstract-full').style.display = 'inline'; document.getElementById('1802.08768v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.08768v2-abstract-full" style="display: none;">
        Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (with z from p(z)) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the &#39;quality&#39; of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a &#39;regularization&#39; technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08768v2-abstract-full').style.display = 'none'; document.getElementById('1802.08768v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.08195">arXiv:1802.08195</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.08195">pdf</a>, <a href="https://arxiv.org/format/1802.08195">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Examples that Fool both Computer Vision and Time-Limited Humans
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elsayed%2C+G+F">Gamaleldin F. Elsayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shankar%2C+S">Shreya Shankar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+B">Brian Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alex Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.08195v3-abstract-short" style="display: inline;">
        Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08195v3-abstract-full').style.display = 'inline'; document.getElementById('1802.08195v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.08195v3-abstract-full" style="display: none;">
        Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08195v3-abstract-full').style.display = 'none'; document.getElementById('1802.08195v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.07736">arXiv:1801.07736</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.07736">pdf</a>, <a href="https://arxiv.org/format/1801.07736">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MaskGAN: Better Text Generation via Filling in the______
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fedus%2C+W">William Fedus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.07736v3-abstract-short" style="display: inline;">
        Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07736v3-abstract-full').style.display = 'inline'; document.getElementById('1801.07736v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.07736v3-abstract-full" style="display: none;">
        Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07736v3-abstract-full').style.display = 'none'; document.getElementById('1801.07736v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, ICLR 2018</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.02774">arXiv:1801.02774</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.02774">pdf</a>, <a href="https://arxiv.org/format/1801.02774">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Spheres
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metz%2C+L">Luke Metz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faghri%2C+F">Fartash Faghri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schoenholz%2C+S+S">Samuel S. Schoenholz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raghu%2C+M">Maithra Raghu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wattenberg%2C+M">Martin Wattenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.02774v3-abstract-short" style="display: inline;">
        State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.02774v3-abstract-full').style.display = 'inline'; document.getElementById('1801.02774v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.02774v3-abstract-full" style="display: none;">
        State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size $O(1/\sqrt{d})$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.02774v3-abstract-full').style.display = 'none'; document.getElementById('1801.02774v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T45
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1710.08446">arXiv:1710.08446</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1710.08446">pdf</a>, <a href="https://arxiv.org/format/1710.08446">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fedus%2C+W">William Fedus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosca%2C+M">Mihaela Rosca</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lakshminarayanan%2C+B">Balaji Lakshminarayanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+S">Shakir Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1710.08446v3-abstract-short" style="display: inline;">
        Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each playe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.08446v3-abstract-full').style.display = 'inline'; document.getElementById('1710.08446v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1710.08446v3-abstract-full" style="display: none;">
        Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players&#39; parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.08446v3-abstract-full').style.display = 'none'; document.getElementById('1710.08446v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 October, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1708.08022">arXiv:1708.08022</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1708.08022">pdf</a>, <a href="https://arxiv.org/ps/1708.08022">ps</a>, <a href="https://arxiv.org/format/1708.08022">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/CSF.2017.10">10.1109/CSF.2017.10 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Protection of Private Information in Machine Learning Systems: Two Recent Approaches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erlingsson%2C+%C3%9A">lfar Erlingsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mironov%2C+I">Ilya Mironov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1708.08022v1-abstract-short" style="display: inline;">
        The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08022v1-abstract-full').style.display = 'inline'; document.getElementById('1708.08022v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1708.08022v1-abstract-full" style="display: none;">
        The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled by Saltzer and Schroeder in the 1970s.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08022v1-abstract-full').style.display = 'none'; document.getElementById('1708.08022v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2017.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE 30th Computer Security Foundations Symposium (CSF), pages 1--6, 2017
      </p>
    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1705.07204">arXiv:1705.07204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1705.07204">pdf</a>, <a href="https://arxiv.org/format/1705.07204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ensemble Adversarial Training: Attacks and Defenses
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tram%C3%A8r%2C+F">Florian Tramr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boneh%2C+D">Dan Boneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1705.07204v4-abstract-short" style="display: inline;">
        Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07204v4-abstract-full').style.display = 'inline'; document.getElementById('1705.07204v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1705.07204v4-abstract-full" style="display: none;">
        Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model&#39;s loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07204v4-abstract-full').style.display = 'none'; document.getElementById('1705.07204v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 May, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 5 figures, International Conference on Learning Representations (ICLR) 2018</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1704.03453">arXiv:1704.03453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1704.03453">pdf</a>, <a href="https://arxiv.org/format/1704.03453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Space of Transferable Adversarial Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tram%C3%A8r%2C+F">Florian Tramr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boneh%2C+D">Dan Boneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1704.03453v2-abstract-short" style="display: inline;">
        Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.03453v2-abstract-full').style.display = 'inline'; document.getElementById('1704.03453v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1704.03453v2-abstract-full" style="display: none;">
        Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability.
  In the first quantitative analysis of the similarity of different models&#39; decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.03453v2-abstract-full').style.display = 'none'; document.getElementById('1704.03453v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 7 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1702.02284">arXiv:1702.02284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1702.02284">pdf</a>, <a href="https://arxiv.org/format/1702.02284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks on Neural Network Policies
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sandy Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+Y">Yan Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abbeel%2C+P">Pieter Abbeel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1702.02284v1-abstract-short" style="display: inline;">
        Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adver&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02284v1-abstract-full').style.display = 'inline'; document.getElementById('1702.02284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1702.02284v1-abstract-full" style="display: none;">
        Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02284v1-abstract-full').style.display = 'none'; document.getElementById('1702.02284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2017.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1701.00160">arXiv:1701.00160</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1701.00160">pdf</a>, <a href="https://arxiv.org/format/1701.00160">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NIPS 2016 Tutorial: Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1701.00160v4-abstract-short" style="display: inline;">
        This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs wi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00160v4-abstract-full').style.display = 'inline'; document.getElementById('1701.00160v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1701.00160v4-abstract-full" style="display: none;">
        This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00160v4-abstract-full').style.display = 'none'; document.getElementById('1701.00160v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 December, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">v2-v4 are all typo fixes. No substantive changes relative to v1</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1611.01236">arXiv:1611.01236</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1611.01236">pdf</a>, <a href="https://arxiv.org/format/1611.01236">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Machine Learning at Scale
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1611.01236v2-abstract-short" style="display: inline;">
        Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01236v2-abstract-full').style.display = 'inline'; document.getElementById('1611.01236v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1611.01236v2-abstract-full" style="display: none;">
        Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model&#39;s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a &#34;label leaking&#34; effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01236v2-abstract-full').style.display = 'none'; document.getElementById('1611.01236v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 November, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 5 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1610.05755">arXiv:1610.05755</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1610.05755">pdf</a>, <a href="https://arxiv.org/format/1610.05755">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erlingsson%2C+%C3%9A">lfar Erlingsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1610.05755v4-abstract-short" style="display: inline;">
        Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.05755v4-abstract-full').style.display = 'inline'; document.getElementById('1610.05755v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1610.05755v4-abstract-full" style="display: none;">
        Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as &#34;teachers&#34; for a &#34;student&#34; model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student&#39;s privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student&#39;s training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings.
  Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.05755v4-abstract-full').style.display = 'none'; document.getElementById('1610.05755v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 October, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICLR 17 as an oral</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1610.00768">arXiv:1610.00768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1610.00768">pdf</a>, <a href="https://arxiv.org/ps/1610.00768">ps</a>, <a href="https://arxiv.org/format/1610.00768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Technical Report on the CleverHans v2.1.0 Adversarial Examples Library
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faghri%2C+F">Fartash Faghri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feinman%2C+R">Reuben Feinman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+Y">Yash Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T">Tom Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roy%2C+A">Aurko Roy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matyasko%2C+A">Alexander Matyasko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behzadan%2C+V">Vahid Behzadan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hambardzumyan%2C+K">Karen Hambardzumyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhishuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Juang%2C+Y">Yi-Lin Juang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sheatsley%2C+R">Ryan Sheatsley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garg%2C+A">Abhibhav Garg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Uesato%2C+J">Jonathan Uesato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gierke%2C+W">Willi Gierke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yinpeng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berthelot%2C+D">David Berthelot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hendricks%2C+P">Paul Hendricks</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rauber%2C+J">Jonas Rauber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+R">Rujun Long</a>
      , et al. (1 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1610.00768v6-abstract-short" style="display: inline;">
        CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial exam&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.00768v6-abstract-full').style.display = 'inline'; document.getElementById('1610.00768v6-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1610.00768v6-abstract-full" style="display: none;">
        CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models&#39; performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure.
  This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the CleverHans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.00768v6-abstract-full').style.display = 'none'; document.getElementById('1610.00768v6-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 October, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical report for https://github.com/tensorflow/cleverhans</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.02533">arXiv:1607.02533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.02533">pdf</a>, <a href="https://arxiv.org/format/1607.02533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial examples in the physical world
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.02533v4-abstract-short" style="display: inline;">
        Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.02533v4-abstract-full').style.display = 'inline'; document.getElementById('1607.02533v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.02533v4-abstract-full" style="display: none;">
        Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.02533v4-abstract-full').style.display = 'none'; document.getElementById('1607.02533v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.00133">arXiv:1607.00133</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.00133">pdf</a>, <a href="https://arxiv.org/format/1607.00133">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/2976749.2978318">10.1145/2976749.2978318 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning with Differential Privacy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+A">Andy Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mironov%2C+I">Ilya Mironov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.00133v2-abstract-short" style="display: inline;">
        Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00133v2-abstract-full').style.display = 'inline'; document.getElementById('1607.00133v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.00133v2-abstract-full" style="display: none;">
        Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00133v2-abstract-full').style.display = 'none'; document.getElementById('1607.00133v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 October, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (ACM CCS), pp. 308-318, 2016
      </p>
    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1606.03498">arXiv:1606.03498</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1606.03498">pdf</a>, <a href="https://arxiv.org/format/1606.03498">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improved Techniques for Training GANs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Salimans%2C+T">Tim Salimans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaremba%2C+W">Wojciech Zaremba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+V">Vicki Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Radford%2C+A">Alec Radford</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xi Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1606.03498v1-abstract-short" style="display: inline;">
        We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, n&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.03498v1-abstract-full').style.display = 'inline'; document.getElementById('1606.03498v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1606.03498v1-abstract-full" style="display: none;">
        We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.03498v1-abstract-full').style.display = 'none'; document.getElementById('1606.03498v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 June, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2016.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07725">arXiv:1605.07725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07725">pdf</a>, <a href="https://arxiv.org/ps/1605.07725">ps</a>, <a href="https://arxiv.org/format/1605.07725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Training Methods for Semi-Supervised Text Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Miyato%2C+T">Takeru Miyato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07725v3-abstract-short" style="display: inline;">
        Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07725v3-abstract-full').style.display = 'inline'; document.getElementById('1605.07725v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07725v3-abstract-full" style="display: none;">
        Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07725v3-abstract-full').style.display = 'none'; document.getElementById('1605.07725v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 May, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2017</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07277">arXiv:1605.07277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07277">pdf</a>, <a href="https://arxiv.org/format/1605.07277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07277v1-abstract-short" style="display: inline;">
        Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07277v1-abstract-full').style.display = 'inline'; document.getElementById('1605.07277v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07277v1-abstract-full" style="display: none;">
        Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07277v1-abstract-full').style.display = 'none'; document.getElementById('1605.07277v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07157">arXiv:1605.07157</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07157">pdf</a>, <a href="https://arxiv.org/format/1605.07157">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Learning for Physical Interaction through Video Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Finn%2C+C">Chelsea Finn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levine%2C+S">Sergey Levine</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07157v4-abstract-short" style="display: inline;">
        A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about ph&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07157v4-abstract-full').style.display = 'inline'; document.getElementById('1605.07157v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07157v4-abstract-full" style="display: none;">
        A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot&#39;s future actions amounts to learning a &#34;visual imagination&#34; of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07157v4-abstract-full').style.display = 'none'; document.getElementById('1605.07157v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 October, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 May, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in NIPS &#39;16; Video results, code, and data available at: http://www.sites.google.com/site/robotprediction</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1604.04326">arXiv:1604.04326</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1604.04326">pdf</a>, <a href="https://arxiv.org/format/1604.04326">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving the Robustness of Deep Neural Networks via Stability Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+S">Stephan Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leung%2C+T">Thomas Leung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1604.04326v1-abstract-short" style="display: inline;">
        In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep network&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.04326v1-abstract-full').style.display = 'inline'; document.getElementById('1604.04326v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1604.04326v1-abstract-full" style="display: none;">
        In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.04326v1-abstract-full').style.display = 'none'; document.getElementById('1604.04326v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in CVPR 2016</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1603.04467">arXiv:1603.04467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1603.04467">pdf</a>, <a href="https://arxiv.org/format/1603.04467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+A">Ashish Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barham%2C+P">Paul Barham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brevdo%2C+E">Eugene Brevdo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhifeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Citro%2C+C">Craig Citro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Corrado%2C+G+S">Greg S. Corrado</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Davis%2C+A">Andy Davis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dean%2C+J">Jeffrey Dean</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Devin%2C+M">Matthieu Devin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghemawat%2C+S">Sanjay Ghemawat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harp%2C+A">Andrew Harp</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Irving%2C+G">Geoffrey Irving</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Isard%2C+M">Michael Isard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+Y">Yangqing Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jozefowicz%2C+R">Rafal Jozefowicz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaiser%2C+L">Lukasz Kaiser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kudlur%2C+M">Manjunath Kudlur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levenberg%2C+J">Josh Levenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mane%2C+D">Dan Mane</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Monga%2C+R">Rajat Monga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moore%2C+S">Sherry Moore</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Murray%2C+D">Derek Murray</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olah%2C+C">Chris Olah</a>
      , et al. (15 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1603.04467v2-abstract-short" style="display: inline;">
        TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1603.04467v2-abstract-full').style.display = 'inline'; document.getElementById('1603.04467v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1603.04467v2-abstract-full" style="display: none;">
        TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1603.04467v2-abstract-full').style.display = 'none'; document.getElementById('1603.04467v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 March, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Version 2 updates only the metadata, to correct the formatting of Martn Abadi&#39;s name</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1602.02697">arXiv:1602.02697</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1602.02697">pdf</a>, <a href="https://arxiv.org/format/1602.02697">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Practical Black-Box Attacks against Machine Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jha%2C+S">Somesh Jha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Celik%2C+Z+B">Z. Berkay Celik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Swami%2C+A">Ananthram Swami</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1602.02697v4-abstract-short" style="display: inline;">
        Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.02697v4-abstract-full').style.display = 'inline'; document.getElementById('1602.02697v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1602.02697v4-abstract-full" style="display: none;">
        Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.02697v4-abstract-full').style.display = 'none'; document.getElementById('1602.02697v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 February, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.05644">arXiv:1511.05644</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.05644">pdf</a>, <a href="https://arxiv.org/format/1511.05644">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Autoencoders
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Makhzani%2C+A">Alireza Makhzani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaitly%2C+N">Navdeep Jaitly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frey%2C+B">Brendan Frey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.05644v2-abstract-short" style="display: inline;">
        In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05644v2-abstract-full').style.display = 'inline'; document.getElementById('1511.05644v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.05644v2-abstract-full" style="display: none;">
        In this paper, we propose the &#34;adversarial autoencoder&#34; (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05644v2-abstract-full').style.display = 'none'; document.getElementById('1511.05644v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.05641">arXiv:1511.05641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.05641">pdf</a>, <a href="https://arxiv.org/format/1511.05641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Net2Net: Accelerating Learning via Knowledge Transfer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tianqi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.05641v4-abstract-short" style="display: inline;">
        We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05641v4-abstract-full').style.display = 'inline'; document.getElementById('1511.05641v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.05641v4-abstract-full" style="display: none;">
        We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05641v4-abstract-full').style.display = 'none'; document.getElementById('1511.05641v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2016 submission</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1510.01799">arXiv:1510.01799</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1510.01799">pdf</a>, <a href="https://arxiv.org/ps/1510.01799">ps</a>, <a href="https://arxiv.org/format/1510.01799">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Per-Example Gradient Computations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1510.01799v2-abstract-short" style="display: inline;">
        This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1510.01799v2-abstract-full" style="display: none;">
        This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1510.01799v2-abstract-full').style.display = 'none'; document.getElementById('1510.01799v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2015; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 October, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This revision fixed some typos. Many thanks to Hugo Larochelle for reporting them!</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1312.6199">arXiv:1312.6199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1312.6199">pdf</a>, <a href="https://arxiv.org/format/1312.6199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intriguing properties of neural networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Szegedy%2C+C">Christian Szegedy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaremba%2C+W">Wojciech Zaremba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sutskever%2C+I">Ilya Sutskever</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bruna%2C+J">Joan Bruna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erhan%2C+D">Dumitru Erhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fergus%2C+R">Rob Fergus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1312.6199v4-abstract-short" style="display: inline;">
        Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction betwee&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1312.6199v4-abstract-full').style.display = 'inline'; document.getElementById('1312.6199v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1312.6199v4-abstract-full" style="display: none;">
        Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.
  Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network&#39;s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1312.6199v4-abstract-full').style.display = 'none'; document.getElementById('1312.6199v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2014; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 December, 2013;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2013.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1212.2686">arXiv:1212.2686</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1212.2686">pdf</a>, <a href="https://arxiv.org/ps/1212.2686">ps</a>, <a href="https://arxiv.org/format/1212.2686">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Training of Deep Boltzmann Machines
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1212.2686v1-abstract-short" style="display: inline;">
        We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1212.2686v1-abstract-full" style="display: none;">
        We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1212.2686v1-abstract-full').style.display = 'none'; document.getElementById('1212.2686v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 December, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1211.5590">arXiv:1211.5590</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1211.5590">pdf</a>, <a href="https://arxiv.org/format/1211.5590">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Symbolic Computation">cs.SC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Theano: new features and speed improvements
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bastien%2C+F">Frdric Bastien</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lamblin%2C+P">Pascal Lamblin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pascanu%2C+R">Razvan Pascanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergstra%2C+J">James Bergstra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergeron%2C+A">Arnaud Bergeron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouchard%2C+N">Nicolas Bouchard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Warde-Farley%2C+D">David Warde-Farley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1211.5590v1-abstract-short" style="display: inline;">
        Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1211.5590v1-abstract-full').style.display = 'inline'; document.getElementById('1211.5590v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1211.5590v1-abstract-full" style="display: none;">
        Theano is a linear algebra compiler that optimizes a user&#39;s symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano&#39;s performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1211.5590v1-abstract-full').style.display = 'none'; document.getElementById('1211.5590v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at the Deep Learning Workshop, NIPS 2012</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1206.6407">arXiv:1206.6407</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1206.6407">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large-Scale Feature Learning With Spike-and-Slab Sparse Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1206.6407v1-abstract-short" style="display: inline;">
        We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enorm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1206.6407v1-abstract-full').style.display = 'inline'; document.getElementById('1206.6407v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1206.6407v1-abstract-full" style="display: none;">
        We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models? Transfer Learning Challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1206.6407v1-abstract-full').style.display = 'none'; document.getElementById('1206.6407v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap with arXiv:1201.3382</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.10346">arXiv:1903.10346</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.10346">pdf</a>, <a href="https://arxiv.org/format/1903.10346">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yao Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cottrell%2C+G">Garrison Cottrell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.10346v1-abstract-short" style="display: inline;">
        Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.10346v1-abstract-full').style.display = 'inline'; document.getElementById('1903.10346v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.10346v1-abstract-full" style="display: none;">
        Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples applied to speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes advances on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.10346v1-abstract-full').style.display = 'none'; document.getElementById('1903.10346v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.06293">arXiv:1903.06293</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.06293">pdf</a>, <a href="https://arxiv.org/ps/1903.06293">ps</a>, <a href="https://arxiv.org/format/1903.06293">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Research Agenda: Dynamic Models to Defend Against Correlated Attacks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.06293v1-abstract-short" style="display: inline;">
        In this article I describe a research agenda for securing machine learning models against adversarial inputs at test time. This article does not present results but instead shares some of my thoughts about where I think that the field needs to go. Modern machine learning works very well on I.I.D. data: data for which each example is drawn {\em independently} and for which the distribution generati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06293v1-abstract-full').style.display = 'inline'; document.getElementById('1903.06293v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.06293v1-abstract-full" style="display: none;">
        In this article I describe a research agenda for securing machine learning models against adversarial inputs at test time. This article does not present results but instead shares some of my thoughts about where I think that the field needs to go. Modern machine learning works very well on I.I.D. data: data for which each example is drawn {\em independently} and for which the distribution generating each example is {\em identical}. When these assumptions are relaxed, modern machine learning can perform very poorly. When machine learning is used in contexts where security is a concern, it is desirable to design models that perform well even when the input is designed by a malicious adversary. So far most research in this direction has focused on an adversary who violates the {\em identical} assumption, and imposes some kind of restricted worst-case distribution shift. I argue that machine learning security researchers should also address the problem of relaxing the {\em independence} assumption and that current strategies designed for robustness to distribution shift will not do so. I recommend {\em dynamic models} that change each time they are run as a potential solution path to this problem, and show an example of a simple attack using correlated data that can be mitigated by a simple dynamic defense. This is not intended as a real-world security measure, but as a recommendation to explore this research direction and develop more realistic defenses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06293v1-abstract-full').style.display = 'none'; document.getElementById('1903.06293v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.06705">arXiv:1902.06705</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.06705">pdf</a>, <a href="https://arxiv.org/ps/1902.06705">ps</a>, <a href="https://arxiv.org/format/1902.06705">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Evaluating Adversarial Robustness
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Athalye%2C+A">Anish Athalye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brendel%2C+W">Wieland Brendel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rauber%2C+J">Jonas Rauber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsipras%2C+D">Dimitris Tsipras</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Madry%2C+A">Aleksander Madry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.06705v2-abstract-short" style="display: inline;">
        Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.
  We believe a large contributing factor is the difficulty of performing security evaluations. In this pa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06705v2-abstract-full').style.display = 'inline'; document.getElementById('1902.06705v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.06705v2-abstract-full" style="display: none;">
        Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.
  We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06705v2-abstract-full').style.display = 'none'; document.getElementById('1902.06705v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 February, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Living document; source available at https://github.com/evaluating-adversarial-robustness/adv-eval-paper/</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.03685">arXiv:1811.03685</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.03685">pdf</a>, <a href="https://arxiv.org/format/1811.03685">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        New CleverHans Feature: Better Adversarial Robustness Evaluations with Attack Bundling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.03685v1-abstract-short" style="display: inline;">
        This technical report describes a new feature of the CleverHans library called "attack bundling". Many papers about adversarial examples present lists of error rates corresponding to different attack algorithms. A common approach is to take the maximum across this list and compare defenses against that error rate. We argue that a better approach is to use attack bundling: the max should be taken a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.03685v1-abstract-full').style.display = 'inline'; document.getElementById('1811.03685v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.03685v1-abstract-full" style="display: none;">
        This technical report describes a new feature of the CleverHans library called &#34;attack bundling&#34;. Many papers about adversarial examples present lists of error rates corresponding to different attack algorithms. A common approach is to take the maximum across this list and compare defenses against that error rate. We argue that a better approach is to use attack bundling: the max should be taken across many examples at the level of individual examples, then the error rate should be calculated by averaging after this maximization operation. Reporting the bundled attacker error rate provides a lower bound on the true worst-case error rate. The traditional approach of reporting the maximum error rate across attacks can underestimate the true worst-case error rate by an amount approaching 100\% as the number of attacks approaches infinity. Attack bundling can be used with different prioritization schemes to optimize quantities such as error rate on adversarial examples, perturbation size needed to cause misclassification, or failure rate when using a specific confidence threshold.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.03685v1-abstract-full').style.display = 'none'; document.getElementById('1811.03685v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.06758">arXiv:1810.06758</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.06758">pdf</a>, <a href="https://arxiv.org/format/1810.06758">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discriminator Rejection Sampling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Azadi%2C+S">Samaneh Azadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Darrell%2C+T">Trevor Darrell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.06758v3-abstract-short" style="display: inline;">
        We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be us&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.06758v3-abstract-full').style.display = 'inline'; document.getElementById('1810.06758v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.06758v3-abstract-full" style="display: none;">
        We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the SAGAN model, state-of-the-art in the image generation task at the time of developing this work. On ImageNet, we train an improved baseline that increases the Inception Score from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.06758v3-abstract-full').style.display = 'none'; document.getElementById('1810.06758v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2019</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03307">arXiv:1810.03307</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03307">pdf</a>, <a href="https://arxiv.org/format/1810.03307">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adebayo%2C+J">Julius Adebayo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Been Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03307v1-abstract-short" style="display: inline;">
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03307v1-abstract-full').style.display = 'inline'; document.getElementById('1810.03307v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03307v1-abstract-full" style="display: none;">
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN&#39;s output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN&#39;s architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et. al. (2018).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03307v1-abstract-full').style.display = 'none'; document.getElementById('1810.03307v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Workshop Track International Conference on Learning Representations (ICLR)</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03292">arXiv:1810.03292</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03292">pdf</a>, <a href="https://arxiv.org/format/1810.03292">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sanity Checks for Saliency Maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adebayo%2C+J">Julius Adebayo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Muelly%2C+M">Michael Muelly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hardt%2C+M">Moritz Hardt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Been Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03292v2-abstract-short" style="display: inline;">
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual ass&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03292v2-abstract-full').style.display = 'inline'; document.getElementById('1810.03292v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03292v2-abstract-full" style="display: none;">
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03292v2-abstract-full').style.display = 'none'; document.getElementById('1810.03292v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NIPS 2018 Camera Ready Version</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.08352">arXiv:1809.08352</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.08352">pdf</a>, <a href="https://arxiv.org/format/1809.08352">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unrestricted Adversarial Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chiyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Christiano%2C+P">Paul Christiano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.08352v1-abstract-short" style="display: inline;">
        We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08352v1-abstract-full').style.display = 'inline'; document.getElementById('1809.08352v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.08352v1-abstract-full" style="display: none;">
        We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. Attackers try to subvert defenses by finding arbitrary unambiguous inputs where the model assigns an incorrect label with high confidence. We propose a simple unambiguous dataset (&#34;bird-or- bicycle&#34;) to use as part of this contest. We hope this contest will help to more comprehensively evaluate the worst-case adversarial risk of machine learning models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08352v1-abstract-full').style.display = 'none'; document.getElementById('1809.08352v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.04888">arXiv:1808.04888</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.04888">pdf</a>, <a href="https://arxiv.org/format/1808.04888">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Skill Rating for Generative Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhupatiraju%2C+S">Surya Bhupatiraju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T">Tom Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.04888v1-abstract-short" style="display: inline;">
        We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different cont&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04888v1-abstract-full').style.display = 'inline'; document.getElementById('1808.04888v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.04888v1-abstract-full" style="display: none;">
        We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04888v1-abstract-full').style.display = 'none'; document.getElementById('1808.04888v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.10875">arXiv:1807.10875</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.10875">pdf</a>, <a href="https://arxiv.org/format/1807.10875">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.10875v1-abstract-short" style="display: inline;">
        Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.10875v1-abstract-full').style.display = 'inline'; document.getElementById('1807.10875v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.10875v1-abstract-full" style="display: none;">
        Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of CGF to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called TensorFuzz that implements the described techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.10875v1-abstract-full').style.display = 'none'; document.getElementById('1807.10875v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint - work in progress</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.07543">arXiv:1807.07543</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.07543">pdf</a>, <a href="https://arxiv.org/format/1807.07543">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Berthelot%2C+D">David Berthelot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roy%2C+A">Aurko Roy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.07543v2-abstract-short" style="display: inline;">
        Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can "interpolate": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.07543v2-abstract-full').style.display = 'inline'; document.getElementById('1807.07543v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.07543v2-abstract-full" style="display: none;">
        Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can &#34;interpolate&#34;: By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.07543v2-abstract-full').style.display = 'none'; document.getElementById('1807.07543v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.06732">arXiv:1807.06732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.06732">pdf</a>, <a href="https://arxiv.org/format/1807.06732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Motivating the Rules of the Game for Adversarial Example Research
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adams%2C+R+P">Ryan P. Adams</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andersen%2C+D">David Andersen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dahl%2C+G+E">George E. Dahl</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.06732v2-abstract-short" style="display: inline;">
        Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06732v2-abstract-full').style.display = 'inline'; document.getElementById('1807.06732v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.06732v2-abstract-full" style="display: none;">
        Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06732v2-abstract-full').style.display = 'none'; document.getElementById('1807.06732v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.11146">arXiv:1806.11146</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.11146">pdf</a>, <a href="https://arxiv.org/format/1806.11146">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Reprogramming of Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elsayed%2C+G+F">Gamaleldin F. Elsayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.11146v2-abstract-short" style="display: inline;">
        Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11146v2-abstract-full').style.display = 'inline'; document.getElementById('1806.11146v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.11146v2-abstract-full" style="display: none;">
        Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11146v2-abstract-full').style.display = 'none'; document.getElementById('1806.11146v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 November, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 June, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.04169">arXiv:1806.04169</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.04169">pdf</a>, <a href="https://arxiv.org/format/1806.04169">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Defense Against the Dark Arts: An overview of adversarial example security research and future research directions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.04169v1-abstract-short" style="display: inline;">
        This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.04169v1-abstract-full" style="display: none;">
        This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.04169v1-abstract-full').style.display = 'none'; document.getElementById('1806.04169v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.08318">arXiv:1805.08318</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.08318">pdf</a>, <a href="https://arxiv.org/format/1805.08318">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Attention Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Han Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metaxas%2C+D">Dimitris Metaxas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.08318v1-abstract-short" style="display: inline;">
        In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08318v1-abstract-full').style.display = 'inline'; document.getElementById('1805.08318v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.08318v1-abstract-full" style="display: none;">
        In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08318v1-abstract-full').style.display = 'none'; document.getElementById('1805.08318v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.07870">arXiv:1804.07870</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.07870">pdf</a>, <a href="https://arxiv.org/format/1804.07870">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.07870v1-abstract-short" style="display: inline;">
        A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.07870v1-abstract-full').style.display = 'inline'; document.getElementById('1804.07870v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.07870v1-abstract-full" style="display: none;">
        A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation that will fool the model, but security guarantees require a lower bound. CLEVER is a proposed scoring method to estimate a lower bound. Unfortunately, an estimate of a bound is not a bound. In this report, we show that gradient masking, a common problem that causes attack methodologies to provide only a very loose upper bound, causes CLEVER to overestimate the size of perturbation needed to fool the model. In other words, CLEVER does not resolve the key problem with the attack-based methodology, because it fails to provide a lower bound.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.07870v1-abstract-full').style.display = 'none'; document.getElementById('1804.07870v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.00097">arXiv:1804.00097</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.00097">pdf</a>, <a href="https://arxiv.org/format/1804.00097">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks and Defences Competition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yinpeng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+F">Fangzhou Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+M">Ming Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+T">Tianyu Pang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jun Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xiaolin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jianyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhishuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Z">Zhou Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuille%2C+A">Alan Yuille</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sangxia Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yuzhe Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhonglin Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+J">Junjiajia Long</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berdibekov%2C+Y">Yerkebulan Berdibekov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akiba%2C+T">Takuya Akiba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tokui%2C+S">Seiya Tokui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abe%2C+M">Motoki Abe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.00097v1-abstract-short" style="display: inline;">
        To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.00097v1-abstract-full').style.display = 'inline'; document.getElementById('1804.00097v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.00097v1-abstract-full" style="display: none;">
        To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.00097v1-abstract-full').style.display = 'none'; document.getElementById('1804.00097v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">36 pages, 10 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.06373">arXiv:1803.06373</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.06373">pdf</a>, <a href="https://arxiv.org/ps/1803.06373">ps</a>, <a href="https://arxiv.org/format/1803.06373">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Logit Pairing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kannan%2C+H">Harini Kannan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.06373v1-abstract-short" style="display: inline;">
        In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06373v1-abstract-full').style.display = 'inline'; document.getElementById('1803.06373v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.06373v1-abstract-full" style="display: none;">
        In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06373v1-abstract-full').style.display = 'none'; document.getElementById('1803.06373v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.08768">arXiv:1802.08768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.08768">pdf</a>, <a href="https://arxiv.org/format/1802.08768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Is Generator Conditioning Causally Related to GAN Performance?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buckman%2C+J">Jacob Buckman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olah%2C+C">Christopher Olah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.08768v2-abstract-short" style="display: inline;">
        Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08768v2-abstract-full').style.display = 'inline'; document.getElementById('1802.08768v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.08768v2-abstract-full" style="display: none;">
        Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (with z from p(z)) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the &#39;quality&#39; of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a &#39;regularization&#39; technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08768v2-abstract-full').style.display = 'none'; document.getElementById('1802.08768v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.08195">arXiv:1802.08195</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.08195">pdf</a>, <a href="https://arxiv.org/format/1802.08195">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Examples that Fool both Computer Vision and Time-Limited Humans
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elsayed%2C+G+F">Gamaleldin F. Elsayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shankar%2C+S">Shreya Shankar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+B">Brian Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alex Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.08195v3-abstract-short" style="display: inline;">
        Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08195v3-abstract-full').style.display = 'inline'; document.getElementById('1802.08195v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.08195v3-abstract-full" style="display: none;">
        Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08195v3-abstract-full').style.display = 'none'; document.getElementById('1802.08195v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.07736">arXiv:1801.07736</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.07736">pdf</a>, <a href="https://arxiv.org/format/1801.07736">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MaskGAN: Better Text Generation via Filling in the______
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fedus%2C+W">William Fedus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.07736v3-abstract-short" style="display: inline;">
        Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07736v3-abstract-full').style.display = 'inline'; document.getElementById('1801.07736v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.07736v3-abstract-full" style="display: none;">
        Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07736v3-abstract-full').style.display = 'none'; document.getElementById('1801.07736v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, ICLR 2018</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.02774">arXiv:1801.02774</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.02774">pdf</a>, <a href="https://arxiv.org/format/1801.02774">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Spheres
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metz%2C+L">Luke Metz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faghri%2C+F">Fartash Faghri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schoenholz%2C+S+S">Samuel S. Schoenholz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raghu%2C+M">Maithra Raghu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wattenberg%2C+M">Martin Wattenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.02774v3-abstract-short" style="display: inline;">
        State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.02774v3-abstract-full').style.display = 'inline'; document.getElementById('1801.02774v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.02774v3-abstract-full" style="display: none;">
        State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size $O(1/\sqrt{d})$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.02774v3-abstract-full').style.display = 'none'; document.getElementById('1801.02774v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T45
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1710.08446">arXiv:1710.08446</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1710.08446">pdf</a>, <a href="https://arxiv.org/format/1710.08446">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fedus%2C+W">William Fedus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosca%2C+M">Mihaela Rosca</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lakshminarayanan%2C+B">Balaji Lakshminarayanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+S">Shakir Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1710.08446v3-abstract-short" style="display: inline;">
        Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each playe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.08446v3-abstract-full').style.display = 'inline'; document.getElementById('1710.08446v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1710.08446v3-abstract-full" style="display: none;">
        Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players&#39; parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.08446v3-abstract-full').style.display = 'none'; document.getElementById('1710.08446v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 October, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1708.08022">arXiv:1708.08022</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1708.08022">pdf</a>, <a href="https://arxiv.org/ps/1708.08022">ps</a>, <a href="https://arxiv.org/format/1708.08022">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/CSF.2017.10">10.1109/CSF.2017.10 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Protection of Private Information in Machine Learning Systems: Two Recent Approaches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erlingsson%2C+%C3%9A">lfar Erlingsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mironov%2C+I">Ilya Mironov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1708.08022v1-abstract-short" style="display: inline;">
        The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08022v1-abstract-full').style.display = 'inline'; document.getElementById('1708.08022v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1708.08022v1-abstract-full" style="display: none;">
        The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled by Saltzer and Schroeder in the 1970s.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08022v1-abstract-full').style.display = 'none'; document.getElementById('1708.08022v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2017.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE 30th Computer Security Foundations Symposium (CSF), pages 1--6, 2017
      </p>
    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1705.07204">arXiv:1705.07204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1705.07204">pdf</a>, <a href="https://arxiv.org/format/1705.07204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ensemble Adversarial Training: Attacks and Defenses
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tram%C3%A8r%2C+F">Florian Tramr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boneh%2C+D">Dan Boneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1705.07204v4-abstract-short" style="display: inline;">
        Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07204v4-abstract-full').style.display = 'inline'; document.getElementById('1705.07204v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1705.07204v4-abstract-full" style="display: none;">
        Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model&#39;s loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07204v4-abstract-full').style.display = 'none'; document.getElementById('1705.07204v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 May, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 5 figures, International Conference on Learning Representations (ICLR) 2018</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1704.03453">arXiv:1704.03453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1704.03453">pdf</a>, <a href="https://arxiv.org/format/1704.03453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Space of Transferable Adversarial Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tram%C3%A8r%2C+F">Florian Tramr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boneh%2C+D">Dan Boneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1704.03453v2-abstract-short" style="display: inline;">
        Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.03453v2-abstract-full').style.display = 'inline'; document.getElementById('1704.03453v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1704.03453v2-abstract-full" style="display: none;">
        Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability.
  In the first quantitative analysis of the similarity of different models&#39; decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.03453v2-abstract-full').style.display = 'none'; document.getElementById('1704.03453v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 7 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1702.02284">arXiv:1702.02284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1702.02284">pdf</a>, <a href="https://arxiv.org/format/1702.02284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks on Neural Network Policies
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sandy Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+Y">Yan Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abbeel%2C+P">Pieter Abbeel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1702.02284v1-abstract-short" style="display: inline;">
        Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adver&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02284v1-abstract-full').style.display = 'inline'; document.getElementById('1702.02284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1702.02284v1-abstract-full" style="display: none;">
        Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02284v1-abstract-full').style.display = 'none'; document.getElementById('1702.02284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2017.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1701.00160">arXiv:1701.00160</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1701.00160">pdf</a>, <a href="https://arxiv.org/format/1701.00160">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NIPS 2016 Tutorial: Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1701.00160v4-abstract-short" style="display: inline;">
        This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs wi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00160v4-abstract-full').style.display = 'inline'; document.getElementById('1701.00160v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1701.00160v4-abstract-full" style="display: none;">
        This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00160v4-abstract-full').style.display = 'none'; document.getElementById('1701.00160v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 December, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">v2-v4 are all typo fixes. No substantive changes relative to v1</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1611.01236">arXiv:1611.01236</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1611.01236">pdf</a>, <a href="https://arxiv.org/format/1611.01236">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Machine Learning at Scale
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1611.01236v2-abstract-short" style="display: inline;">
        Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01236v2-abstract-full').style.display = 'inline'; document.getElementById('1611.01236v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1611.01236v2-abstract-full" style="display: none;">
        Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model&#39;s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a &#34;label leaking&#34; effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01236v2-abstract-full').style.display = 'none'; document.getElementById('1611.01236v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 November, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 5 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1610.05755">arXiv:1610.05755</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1610.05755">pdf</a>, <a href="https://arxiv.org/format/1610.05755">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erlingsson%2C+%C3%9A">lfar Erlingsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1610.05755v4-abstract-short" style="display: inline;">
        Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.05755v4-abstract-full').style.display = 'inline'; document.getElementById('1610.05755v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1610.05755v4-abstract-full" style="display: none;">
        Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as &#34;teachers&#34; for a &#34;student&#34; model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student&#39;s privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student&#39;s training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings.
  Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.05755v4-abstract-full').style.display = 'none'; document.getElementById('1610.05755v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 October, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICLR 17 as an oral</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1610.00768">arXiv:1610.00768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1610.00768">pdf</a>, <a href="https://arxiv.org/ps/1610.00768">ps</a>, <a href="https://arxiv.org/format/1610.00768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Technical Report on the CleverHans v2.1.0 Adversarial Examples Library
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faghri%2C+F">Fartash Faghri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feinman%2C+R">Reuben Feinman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+Y">Yash Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T">Tom Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roy%2C+A">Aurko Roy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matyasko%2C+A">Alexander Matyasko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behzadan%2C+V">Vahid Behzadan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hambardzumyan%2C+K">Karen Hambardzumyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhishuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Juang%2C+Y">Yi-Lin Juang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sheatsley%2C+R">Ryan Sheatsley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garg%2C+A">Abhibhav Garg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Uesato%2C+J">Jonathan Uesato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gierke%2C+W">Willi Gierke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yinpeng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berthelot%2C+D">David Berthelot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hendricks%2C+P">Paul Hendricks</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rauber%2C+J">Jonas Rauber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+R">Rujun Long</a>
      , et al. (1 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1610.00768v6-abstract-short" style="display: inline;">
        CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial exam&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.00768v6-abstract-full').style.display = 'inline'; document.getElementById('1610.00768v6-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1610.00768v6-abstract-full" style="display: none;">
        CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models&#39; performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure.
  This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the CleverHans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.00768v6-abstract-full').style.display = 'none'; document.getElementById('1610.00768v6-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 October, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical report for https://github.com/tensorflow/cleverhans</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.02533">arXiv:1607.02533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.02533">pdf</a>, <a href="https://arxiv.org/format/1607.02533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial examples in the physical world
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.02533v4-abstract-short" style="display: inline;">
        Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.02533v4-abstract-full').style.display = 'inline'; document.getElementById('1607.02533v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.02533v4-abstract-full" style="display: none;">
        Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.02533v4-abstract-full').style.display = 'none'; document.getElementById('1607.02533v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.00133">arXiv:1607.00133</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.00133">pdf</a>, <a href="https://arxiv.org/format/1607.00133">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/2976749.2978318">10.1145/2976749.2978318 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning with Differential Privacy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+A">Andy Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mironov%2C+I">Ilya Mironov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.00133v2-abstract-short" style="display: inline;">
        Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00133v2-abstract-full').style.display = 'inline'; document.getElementById('1607.00133v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.00133v2-abstract-full" style="display: none;">
        Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00133v2-abstract-full').style.display = 'none'; document.getElementById('1607.00133v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 October, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (ACM CCS), pp. 308-318, 2016
      </p>
    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1606.03498">arXiv:1606.03498</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1606.03498">pdf</a>, <a href="https://arxiv.org/format/1606.03498">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improved Techniques for Training GANs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Salimans%2C+T">Tim Salimans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaremba%2C+W">Wojciech Zaremba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+V">Vicki Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Radford%2C+A">Alec Radford</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xi Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1606.03498v1-abstract-short" style="display: inline;">
        We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, n&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.03498v1-abstract-full').style.display = 'inline'; document.getElementById('1606.03498v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1606.03498v1-abstract-full" style="display: none;">
        We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.03498v1-abstract-full').style.display = 'none'; document.getElementById('1606.03498v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 June, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2016.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07725">arXiv:1605.07725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07725">pdf</a>, <a href="https://arxiv.org/ps/1605.07725">ps</a>, <a href="https://arxiv.org/format/1605.07725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Training Methods for Semi-Supervised Text Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Miyato%2C+T">Takeru Miyato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07725v3-abstract-short" style="display: inline;">
        Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07725v3-abstract-full').style.display = 'inline'; document.getElementById('1605.07725v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07725v3-abstract-full" style="display: none;">
        Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07725v3-abstract-full').style.display = 'none'; document.getElementById('1605.07725v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 May, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2017</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07277">arXiv:1605.07277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07277">pdf</a>, <a href="https://arxiv.org/format/1605.07277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07277v1-abstract-short" style="display: inline;">
        Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07277v1-abstract-full').style.display = 'inline'; document.getElementById('1605.07277v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07277v1-abstract-full" style="display: none;">
        Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07277v1-abstract-full').style.display = 'none'; document.getElementById('1605.07277v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07157">arXiv:1605.07157</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07157">pdf</a>, <a href="https://arxiv.org/format/1605.07157">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Learning for Physical Interaction through Video Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Finn%2C+C">Chelsea Finn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levine%2C+S">Sergey Levine</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07157v4-abstract-short" style="display: inline;">
        A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about ph&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07157v4-abstract-full').style.display = 'inline'; document.getElementById('1605.07157v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07157v4-abstract-full" style="display: none;">
        A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot&#39;s future actions amounts to learning a &#34;visual imagination&#34; of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07157v4-abstract-full').style.display = 'none'; document.getElementById('1605.07157v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 October, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 May, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in NIPS &#39;16; Video results, code, and data available at: http://www.sites.google.com/site/robotprediction</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1604.04326">arXiv:1604.04326</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1604.04326">pdf</a>, <a href="https://arxiv.org/format/1604.04326">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving the Robustness of Deep Neural Networks via Stability Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+S">Stephan Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leung%2C+T">Thomas Leung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1604.04326v1-abstract-short" style="display: inline;">
        In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep network&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.04326v1-abstract-full').style.display = 'inline'; document.getElementById('1604.04326v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1604.04326v1-abstract-full" style="display: none;">
        In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.04326v1-abstract-full').style.display = 'none'; document.getElementById('1604.04326v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in CVPR 2016</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1603.04467">arXiv:1603.04467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1603.04467">pdf</a>, <a href="https://arxiv.org/format/1603.04467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+A">Ashish Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barham%2C+P">Paul Barham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brevdo%2C+E">Eugene Brevdo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhifeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Citro%2C+C">Craig Citro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Corrado%2C+G+S">Greg S. Corrado</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Davis%2C+A">Andy Davis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dean%2C+J">Jeffrey Dean</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Devin%2C+M">Matthieu Devin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghemawat%2C+S">Sanjay Ghemawat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harp%2C+A">Andrew Harp</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Irving%2C+G">Geoffrey Irving</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Isard%2C+M">Michael Isard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+Y">Yangqing Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jozefowicz%2C+R">Rafal Jozefowicz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaiser%2C+L">Lukasz Kaiser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kudlur%2C+M">Manjunath Kudlur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levenberg%2C+J">Josh Levenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mane%2C+D">Dan Mane</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Monga%2C+R">Rajat Monga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moore%2C+S">Sherry Moore</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Murray%2C+D">Derek Murray</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olah%2C+C">Chris Olah</a>
      , et al. (15 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1603.04467v2-abstract-short" style="display: inline;">
        TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1603.04467v2-abstract-full').style.display = 'inline'; document.getElementById('1603.04467v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1603.04467v2-abstract-full" style="display: none;">
        TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1603.04467v2-abstract-full').style.display = 'none'; document.getElementById('1603.04467v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 March, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Version 2 updates only the metadata, to correct the formatting of Martn Abadi&#39;s name</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1602.02697">arXiv:1602.02697</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1602.02697">pdf</a>, <a href="https://arxiv.org/format/1602.02697">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Practical Black-Box Attacks against Machine Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jha%2C+S">Somesh Jha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Celik%2C+Z+B">Z. Berkay Celik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Swami%2C+A">Ananthram Swami</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1602.02697v4-abstract-short" style="display: inline;">
        Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.02697v4-abstract-full').style.display = 'inline'; document.getElementById('1602.02697v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1602.02697v4-abstract-full" style="display: none;">
        Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.02697v4-abstract-full').style.display = 'none'; document.getElementById('1602.02697v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 February, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.05644">arXiv:1511.05644</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.05644">pdf</a>, <a href="https://arxiv.org/format/1511.05644">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Autoencoders
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Makhzani%2C+A">Alireza Makhzani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaitly%2C+N">Navdeep Jaitly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frey%2C+B">Brendan Frey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.05644v2-abstract-short" style="display: inline;">
        In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05644v2-abstract-full').style.display = 'inline'; document.getElementById('1511.05644v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.05644v2-abstract-full" style="display: none;">
        In this paper, we propose the &#34;adversarial autoencoder&#34; (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05644v2-abstract-full').style.display = 'none'; document.getElementById('1511.05644v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.05641">arXiv:1511.05641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.05641">pdf</a>, <a href="https://arxiv.org/format/1511.05641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Net2Net: Accelerating Learning via Knowledge Transfer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tianqi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.05641v4-abstract-short" style="display: inline;">
        We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05641v4-abstract-full').style.display = 'inline'; document.getElementById('1511.05641v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.05641v4-abstract-full" style="display: none;">
        We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05641v4-abstract-full').style.display = 'none'; document.getElementById('1511.05641v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2016 submission</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1510.01799">arXiv:1510.01799</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1510.01799">pdf</a>, <a href="https://arxiv.org/ps/1510.01799">ps</a>, <a href="https://arxiv.org/format/1510.01799">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Per-Example Gradient Computations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1510.01799v2-abstract-short" style="display: inline;">
        This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1510.01799v2-abstract-full" style="display: none;">
        This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1510.01799v2-abstract-full').style.display = 'none'; document.getElementById('1510.01799v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2015; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 October, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This revision fixed some typos. Many thanks to Hugo Larochelle for reporting them!</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1312.6199">arXiv:1312.6199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1312.6199">pdf</a>, <a href="https://arxiv.org/format/1312.6199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intriguing properties of neural networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Szegedy%2C+C">Christian Szegedy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaremba%2C+W">Wojciech Zaremba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sutskever%2C+I">Ilya Sutskever</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bruna%2C+J">Joan Bruna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erhan%2C+D">Dumitru Erhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fergus%2C+R">Rob Fergus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1312.6199v4-abstract-short" style="display: inline;">
        Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction betwee&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1312.6199v4-abstract-full').style.display = 'inline'; document.getElementById('1312.6199v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1312.6199v4-abstract-full" style="display: none;">
        Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.
  Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network&#39;s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1312.6199v4-abstract-full').style.display = 'none'; document.getElementById('1312.6199v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2014; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 December, 2013;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2013.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1212.2686">arXiv:1212.2686</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1212.2686">pdf</a>, <a href="https://arxiv.org/ps/1212.2686">ps</a>, <a href="https://arxiv.org/format/1212.2686">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Training of Deep Boltzmann Machines
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1212.2686v1-abstract-short" style="display: inline;">
        We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1212.2686v1-abstract-full" style="display: none;">
        We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1212.2686v1-abstract-full').style.display = 'none'; document.getElementById('1212.2686v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 December, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1211.5590">arXiv:1211.5590</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1211.5590">pdf</a>, <a href="https://arxiv.org/format/1211.5590">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Symbolic Computation">cs.SC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Theano: new features and speed improvements
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bastien%2C+F">Frdric Bastien</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lamblin%2C+P">Pascal Lamblin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pascanu%2C+R">Razvan Pascanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergstra%2C+J">James Bergstra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergeron%2C+A">Arnaud Bergeron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouchard%2C+N">Nicolas Bouchard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Warde-Farley%2C+D">David Warde-Farley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1211.5590v1-abstract-short" style="display: inline;">
        Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1211.5590v1-abstract-full').style.display = 'inline'; document.getElementById('1211.5590v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1211.5590v1-abstract-full" style="display: none;">
        Theano is a linear algebra compiler that optimizes a user&#39;s symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano&#39;s performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1211.5590v1-abstract-full').style.display = 'none'; document.getElementById('1211.5590v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at the Deep Learning Workshop, NIPS 2012</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1206.6407">arXiv:1206.6407</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1206.6407">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large-Scale Feature Learning With Spike-and-Slab Sparse Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1206.6407v1-abstract-short" style="display: inline;">
        We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enorm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1206.6407v1-abstract-full').style.display = 'inline'; document.getElementById('1206.6407v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1206.6407v1-abstract-full" style="display: none;">
        We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models? Transfer Learning Challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1206.6407v1-abstract-full').style.display = 'none'; document.getElementById('1206.6407v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap with arXiv:1201.3382</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.10346">arXiv:1903.10346</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.10346">pdf</a>, <a href="https://arxiv.org/format/1903.10346">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yao Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cottrell%2C+G">Garrison Cottrell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.10346v1-abstract-short" style="display: inline;">
        Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.10346v1-abstract-full').style.display = 'inline'; document.getElementById('1903.10346v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.10346v1-abstract-full" style="display: none;">
        Adversarial examples are inputs to machine learning models designed by an adversary to cause an incorrect output. So far, adversarial examples have been studied most extensively in the image domain. In this domain, adversarial examples can be constructed by imperceptibly modifying images to cause misclassification, and are practical in the physical world. In contrast, current targeted adversarial examples applied to speech recognition systems have neither of these properties: humans can easily identify the adversarial perturbations, and they are not effective when played over-the-air. This paper makes advances on both of these fronts. First, we develop effectively imperceptible audio adversarial examples (verified through a human study) by leveraging the psychoacoustic principle of auditory masking, while retaining 100% targeted success rate on arbitrary full-sentence targets. Next, we make progress towards physical-world over-the-air audio adversarial examples by constructing perturbations which remain effective even after applying realistic simulated environmental distortions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.10346v1-abstract-full').style.display = 'none'; document.getElementById('1903.10346v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.06293">arXiv:1903.06293</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.06293">pdf</a>, <a href="https://arxiv.org/ps/1903.06293">ps</a>, <a href="https://arxiv.org/format/1903.06293">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Research Agenda: Dynamic Models to Defend Against Correlated Attacks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.06293v1-abstract-short" style="display: inline;">
        In this article I describe a research agenda for securing machine learning models against adversarial inputs at test time. This article does not present results but instead shares some of my thoughts about where I think that the field needs to go. Modern machine learning works very well on I.I.D. data: data for which each example is drawn {\em independently} and for which the distribution generati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06293v1-abstract-full').style.display = 'inline'; document.getElementById('1903.06293v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.06293v1-abstract-full" style="display: none;">
        In this article I describe a research agenda for securing machine learning models against adversarial inputs at test time. This article does not present results but instead shares some of my thoughts about where I think that the field needs to go. Modern machine learning works very well on I.I.D. data: data for which each example is drawn {\em independently} and for which the distribution generating each example is {\em identical}. When these assumptions are relaxed, modern machine learning can perform very poorly. When machine learning is used in contexts where security is a concern, it is desirable to design models that perform well even when the input is designed by a malicious adversary. So far most research in this direction has focused on an adversary who violates the {\em identical} assumption, and imposes some kind of restricted worst-case distribution shift. I argue that machine learning security researchers should also address the problem of relaxing the {\em independence} assumption and that current strategies designed for robustness to distribution shift will not do so. I recommend {\em dynamic models} that change each time they are run as a potential solution path to this problem, and show an example of a simple attack using correlated data that can be mitigated by a simple dynamic defense. This is not intended as a real-world security measure, but as a recommendation to explore this research direction and develop more realistic defenses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.06293v1-abstract-full').style.display = 'none'; document.getElementById('1903.06293v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.06705">arXiv:1902.06705</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.06705">pdf</a>, <a href="https://arxiv.org/ps/1902.06705">ps</a>, <a href="https://arxiv.org/format/1902.06705">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On Evaluating Adversarial Robustness
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Athalye%2C+A">Anish Athalye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brendel%2C+W">Wieland Brendel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rauber%2C+J">Jonas Rauber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsipras%2C+D">Dimitris Tsipras</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Madry%2C+A">Aleksander Madry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.06705v2-abstract-short" style="display: inline;">
        Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.
  We believe a large contributing factor is the difficulty of performing security evaluations. In this pa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06705v2-abstract-full').style.display = 'inline'; document.getElementById('1902.06705v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.06705v2-abstract-full" style="display: none;">
        Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect.
  We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.06705v2-abstract-full').style.display = 'none'; document.getElementById('1902.06705v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 February, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Living document; source available at https://github.com/evaluating-adversarial-robustness/adv-eval-paper/</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.03685">arXiv:1811.03685</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.03685">pdf</a>, <a href="https://arxiv.org/format/1811.03685">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        New CleverHans Feature: Better Adversarial Robustness Evaluations with Attack Bundling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.03685v1-abstract-short" style="display: inline;">
        This technical report describes a new feature of the CleverHans library called "attack bundling". Many papers about adversarial examples present lists of error rates corresponding to different attack algorithms. A common approach is to take the maximum across this list and compare defenses against that error rate. We argue that a better approach is to use attack bundling: the max should be taken a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.03685v1-abstract-full').style.display = 'inline'; document.getElementById('1811.03685v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.03685v1-abstract-full" style="display: none;">
        This technical report describes a new feature of the CleverHans library called &#34;attack bundling&#34;. Many papers about adversarial examples present lists of error rates corresponding to different attack algorithms. A common approach is to take the maximum across this list and compare defenses against that error rate. We argue that a better approach is to use attack bundling: the max should be taken across many examples at the level of individual examples, then the error rate should be calculated by averaging after this maximization operation. Reporting the bundled attacker error rate provides a lower bound on the true worst-case error rate. The traditional approach of reporting the maximum error rate across attacks can underestimate the true worst-case error rate by an amount approaching 100\% as the number of attacks approaches infinity. Attack bundling can be used with different prioritization schemes to optimize quantities such as error rate on adversarial examples, perturbation size needed to cause misclassification, or failure rate when using a specific confidence threshold.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.03685v1-abstract-full').style.display = 'none'; document.getElementById('1811.03685v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.06758">arXiv:1810.06758</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.06758">pdf</a>, <a href="https://arxiv.org/format/1810.06758">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discriminator Rejection Sampling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Azadi%2C+S">Samaneh Azadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Darrell%2C+T">Trevor Darrell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.06758v3-abstract-short" style="display: inline;">
        We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be us&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.06758v3-abstract-full').style.display = 'inline'; document.getElementById('1810.06758v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.06758v3-abstract-full" style="display: none;">
        We propose a rejection sampling scheme using the discriminator of a GAN to approximately correct errors in the GAN generator distribution. We show that under quite strict assumptions, this will allow us to recover the data distribution exactly. We then examine where those strict assumptions break down and design a practical algorithm - called Discriminator Rejection Sampling (DRS) - that can be used on real data-sets. Finally, we demonstrate the efficacy of DRS on a mixture of Gaussians and on the SAGAN model, state-of-the-art in the image generation task at the time of developing this work. On ImageNet, we train an improved baseline that increases the Inception Score from 52.52 to 62.36 and reduces the Frechet Inception Distance from 18.65 to 14.79. We then use DRS to further improve on this baseline, improving the Inception Score to 76.08 and the FID to 13.75.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.06758v3-abstract-full').style.display = 'none'; document.getElementById('1810.06758v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2019</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03307">arXiv:1810.03307</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03307">pdf</a>, <a href="https://arxiv.org/format/1810.03307">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Local Explanation Methods for Deep Neural Networks Lack Sensitivity to Parameter Values
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adebayo%2C+J">Julius Adebayo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Been Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03307v1-abstract-short" style="display: inline;">
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN's output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03307v1-abstract-full').style.display = 'inline'; document.getElementById('1810.03307v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03307v1-abstract-full" style="display: none;">
        Explaining the output of a complicated machine learning model like a deep neural network (DNN) is a central challenge in machine learning. Several proposed local explanation methods address this issue by identifying what dimensions of a single input are most responsible for a DNN&#39;s output. The goal of this work is to assess the sensitivity of local explanations to DNN parameter values. Somewhat surprisingly, we find that DNNs with randomly-initialized weights produce explanations that are both visually and quantitatively similar to those produced by DNNs with learned weights. Our conjecture is that this phenomenon occurs because these explanations are dominated by the lower level features of a DNN, and that a DNN&#39;s architecture provides a strong prior which significantly affects the representations learned at these lower layers. NOTE: This work is now subsumed by our recent manuscript, Sanity Checks for Saliency Maps (to appear NIPS 2018), where we expand on findings and address concerns raised in Sundararajan et. al. (2018).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03307v1-abstract-full').style.display = 'none'; document.getElementById('1810.03307v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Workshop Track International Conference on Learning Representations (ICLR)</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1810.03292">arXiv:1810.03292</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1810.03292">pdf</a>, <a href="https://arxiv.org/format/1810.03292">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sanity Checks for Saliency Maps
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Adebayo%2C+J">Julius Adebayo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Muelly%2C+M">Michael Muelly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hardt%2C+M">Moritz Hardt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+B">Been Kim</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1810.03292v2-abstract-short" style="display: inline;">
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual ass&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03292v2-abstract-full').style.display = 'inline'; document.getElementById('1810.03292v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1810.03292v2-abstract-full" style="display: none;">
        Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1810.03292v2-abstract-full').style.display = 'none'; document.getElementById('1810.03292v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 October, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NIPS 2018 Camera Ready Version</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.08352">arXiv:1809.08352</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.08352">pdf</a>, <a href="https://arxiv.org/format/1809.08352">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unrestricted Adversarial Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chiyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Christiano%2C+P">Paul Christiano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.08352v1-abstract-short" style="display: inline;">
        We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08352v1-abstract-full').style.display = 'inline'; document.getElementById('1809.08352v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.08352v1-abstract-full" style="display: none;">
        We introduce a two-player contest for evaluating the safety and robustness of machine learning systems, with a large prize pool. Unlike most prior work in ML robustness, which studies norm-constrained adversaries, we shift our focus to unconstrained adversaries. Defenders submit machine learning models, and try to achieve high accuracy and coverage on non-adversarial data while making no confident mistakes on adversarial inputs. Attackers try to subvert defenses by finding arbitrary unambiguous inputs where the model assigns an incorrect label with high confidence. We propose a simple unambiguous dataset (&#34;bird-or- bicycle&#34;) to use as part of this contest. We hope this contest will help to more comprehensively evaluate the worst-case adversarial risk of machine learning models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.08352v1-abstract-full').style.display = 'none'; document.getElementById('1809.08352v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1808.04888">arXiv:1808.04888</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1808.04888">pdf</a>, <a href="https://arxiv.org/format/1808.04888">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Skill Rating for Generative Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhupatiraju%2C+S">Surya Bhupatiraju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T">Tom Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1808.04888v1-abstract-short" style="display: inline;">
        We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different cont&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04888v1-abstract-full').style.display = 'inline'; document.getElementById('1808.04888v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1808.04888v1-abstract-full" style="display: none;">
        We explore a new way to evaluate generative models using insights from evaluation of competitive games between human players. We show experimentally that tournaments between generators and discriminators provide an effective way to evaluate generative models. We introduce two methods for summarizing tournament outcomes: tournament win rate and skill rating. Evaluations are useful in different contexts, including monitoring the progress of a single model as it learns during the training process, and comparing the capabilities of two different fully trained models. We show that a tournament consisting of a single model playing against past and future versions of itself produces a useful measure of training progress. A tournament containing multiple separate models (using different seeds, hyperparameters, and architectures) provides a useful relative comparison between different trained GANs. Tournament-based rating methods are conceptually distinct from numerous previous categories of approaches to evaluation of generative models, and have complementary advantages and disadvantages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1808.04888v1-abstract-full').style.display = 'none'; document.getElementById('1808.04888v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.10875">arXiv:1807.10875</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.10875">pdf</a>, <a href="https://arxiv.org/format/1807.10875">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.10875v1-abstract-short" style="display: inline;">
        Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.10875v1-abstract-full').style.display = 'inline'; document.getElementById('1807.10875v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.10875v1-abstract-full" style="display: none;">
        Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of CGF to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called TensorFuzz that implements the described techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.10875v1-abstract-full').style.display = 'none'; document.getElementById('1807.10875v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint - work in progress</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.07543">arXiv:1807.07543</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.07543">pdf</a>, <a href="https://arxiv.org/format/1807.07543">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Berthelot%2C+D">David Berthelot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roy%2C+A">Aurko Roy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.07543v2-abstract-short" style="display: inline;">
        Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can "interpolate": By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.07543v2-abstract-full').style.display = 'inline'; document.getElementById('1807.07543v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.07543v2-abstract-full" style="display: none;">
        Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can &#34;interpolate&#34;: By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.07543v2-abstract-full').style.display = 'none'; document.getElementById('1807.07543v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.06732">arXiv:1807.06732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.06732">pdf</a>, <a href="https://arxiv.org/format/1807.06732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Motivating the Rules of the Game for Adversarial Example Research
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adams%2C+R+P">Ryan P. Adams</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andersen%2C+D">David Andersen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dahl%2C+G+E">George E. Dahl</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.06732v2-abstract-short" style="display: inline;">
        Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06732v2-abstract-full').style.display = 'inline'; document.getElementById('1807.06732v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.06732v2-abstract-full" style="display: none;">
        Advances in machine learning have led to broad deployment of systems with impressive performance on important problems. Nonetheless, these systems can be induced to make errors on data that are surprisingly similar to examples the learned system handles correctly. The existence of these errors raises a variety of questions about out-of-sample generalization and whether bad actors might use such examples to abuse deployed systems. As a result of these security concerns, there has been a flurry of recent papers proposing algorithms to defend against such malicious perturbations of correctly handled examples. It is unclear how such misclassifications represent a different kind of security problem than other errors, or even other attacker-produced examples that have no specific relationship to an uncorrupted input. In this paper, we argue that adversarial example defense papers have, to date, mostly considered abstract, toy games that do not relate to any specific security concern. Furthermore, defense papers have not yet precisely described all the abilities and limitations of attackers that would be relevant in practical security. Towards this end, we establish a taxonomy of motivations, constraints, and abilities for more plausible adversaries. Finally, we provide a series of recommendations outlining a path forward for future work to more clearly articulate the threat model and perform more meaningful evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06732v2-abstract-full').style.display = 'none'; document.getElementById('1807.06732v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.11146">arXiv:1806.11146</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.11146">pdf</a>, <a href="https://arxiv.org/format/1806.11146">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Reprogramming of Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elsayed%2C+G+F">Gamaleldin F. Elsayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.11146v2-abstract-short" style="display: inline;">
        Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11146v2-abstract-full').style.display = 'inline'; document.getElementById('1806.11146v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.11146v2-abstract-full" style="display: none;">
        Deep neural networks are susceptible to \emph{adversarial} attacks. In computer vision, well-crafted perturbations to images can cause neural networks to make mistakes such as confusing a cat with a computer. Previous adversarial attacks have been designed to degrade performance of models or cause machine learning models to produce specific outputs chosen ahead of time by the attacker. We introduce attacks that instead {\em reprogram} the target model to perform a task chosen by the attacker---without the attacker needing to specify or compute the desired output for each test-time input. This attack finds a single adversarial perturbation, that can be added to all test-time inputs to a machine learning model in order to cause the model to perform a task chosen by the adversary---even if the model was not trained to do this task. These perturbations can thus be considered a program for the new task. We demonstrate adversarial reprogramming on six ImageNet classification models, repurposing these models to perform a counting task, as well as classification tasks: classification of MNIST and CIFAR-10 examples presented as inputs to the ImageNet model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.11146v2-abstract-full').style.display = 'none'; document.getElementById('1806.11146v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 November, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 June, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1806.04169">arXiv:1806.04169</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1806.04169">pdf</a>, <a href="https://arxiv.org/format/1806.04169">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Defense Against the Dark Arts: An overview of adversarial example security research and future research directions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1806.04169v1-abstract-short" style="display: inline;">
        This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1806.04169v1-abstract-full" style="display: none;">
        This article presents a summary of a keynote lecture at the Deep Learning Security workshop at IEEE Security and Privacy 2018. This lecture summarizes the state of the art in defenses against adversarial examples and provides recommendations for future research directions on this topic.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1806.04169v1-abstract-full').style.display = 'none'; document.getElementById('1806.04169v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.08318">arXiv:1805.08318</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.08318">pdf</a>, <a href="https://arxiv.org/format/1805.08318">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Attention Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Han Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metaxas%2C+D">Dimitris Metaxas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.08318v1-abstract-short" style="display: inline;">
        In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08318v1-abstract-full').style.display = 'inline'; document.getElementById('1805.08318v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.08318v1-abstract-full" style="display: none;">
        In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.08318v1-abstract-full').style.display = 'none'; document.getElementById('1805.08318v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.07870">arXiv:1804.07870</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.07870">pdf</a>, <a href="https://arxiv.org/format/1804.07870">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gradient Masking Causes CLEVER to Overestimate Adversarial Perturbation Size
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.07870v1-abstract-short" style="display: inline;">
        A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.07870v1-abstract-full').style.display = 'inline'; document.getElementById('1804.07870v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.07870v1-abstract-full" style="display: none;">
        A key problem in research on adversarial examples is that vulnerability to adversarial examples is usually measured by running attack algorithms. Because the attack algorithms are not optimal, the attack algorithms are prone to overestimating the size of perturbation needed to fool the target model. In other words, the attack-based methodology provides an upper-bound on the size of a perturbation that will fool the model, but security guarantees require a lower bound. CLEVER is a proposed scoring method to estimate a lower bound. Unfortunately, an estimate of a bound is not a bound. In this report, we show that gradient masking, a common problem that causes attack methodologies to provide only a very loose upper bound, causes CLEVER to overestimate the size of perturbation needed to fool the model. In other words, CLEVER does not resolve the key problem with the attack-based methodology, because it fails to provide a lower bound.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.07870v1-abstract-full').style.display = 'none'; document.getElementById('1804.07870v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.00097">arXiv:1804.00097</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.00097">pdf</a>, <a href="https://arxiv.org/format/1804.00097">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks and Defences Competition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yinpeng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+F">Fangzhou Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+M">Ming Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+T">Tianyu Pang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jun Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xiaolin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jianyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhishuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Z">Zhou Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuille%2C+A">Alan Yuille</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sangxia Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yuzhe Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhonglin Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+J">Junjiajia Long</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berdibekov%2C+Y">Yerkebulan Berdibekov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akiba%2C+T">Takuya Akiba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tokui%2C+S">Seiya Tokui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abe%2C+M">Motoki Abe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.00097v1-abstract-short" style="display: inline;">
        To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.00097v1-abstract-full').style.display = 'inline'; document.getElementById('1804.00097v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.00097v1-abstract-full" style="display: none;">
        To accelerate research on adversarial examples and robustness of machine learning classifiers, Google Brain organized a NIPS 2017 competition that encouraged researchers to develop new methods to generate adversarial examples as well as to develop new ways to defend against them. In this chapter, we describe the structure and organization of the competition and the solutions developed by several of the top-placing teams.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.00097v1-abstract-full').style.display = 'none'; document.getElementById('1804.00097v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">36 pages, 10 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1803.06373">arXiv:1803.06373</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1803.06373">pdf</a>, <a href="https://arxiv.org/ps/1803.06373">ps</a>, <a href="https://arxiv.org/format/1803.06373">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Logit Pairing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kannan%2C+H">Harini Kannan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1803.06373v1-abstract-short" style="display: inline;">
        In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06373v1-abstract-full').style.display = 'inline'; document.getElementById('1803.06373v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1803.06373v1-abstract-full" style="display: none;">
        In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1803.06373v1-abstract-full').style.display = 'none'; document.getElementById('1803.06373v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.08768">arXiv:1802.08768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.08768">pdf</a>, <a href="https://arxiv.org/format/1802.08768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Is Generator Conditioning Causally Related to GAN Performance?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Odena%2C+A">Augustus Odena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buckman%2C+J">Jacob Buckman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olsson%2C+C">Catherine Olsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T+B">Tom B. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olah%2C+C">Christopher Olah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raffel%2C+C">Colin Raffel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.08768v2-abstract-short" style="display: inline;">
        Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08768v2-abstract-full').style.display = 'inline'; document.getElementById('1802.08768v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.08768v2-abstract-full" style="display: none;">
        Recent work (Pennington et al, 2017) suggests that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning. Motivated by this, we study the distribution of singular values of the Jacobian of the generator in Generative Adversarial Networks (GANs). We find that this Jacobian generally becomes ill-conditioned at the beginning of training. Moreover, we find that the average (with z from p(z)) conditioning of the generator is highly predictive of two other ad-hoc metrics for measuring the &#39;quality&#39; of trained GANs: the Inception Score and the Frechet Inception Distance (FID). We test the hypothesis that this relationship is causal by proposing a &#39;regularization&#39; technique (called Jacobian Clamping) that softly penalizes the condition number of the generator Jacobian. Jacobian Clamping improves the mean Inception Score and the mean FID for GANs trained on several datasets. It also greatly reduces inter-run variance of the aforementioned scores, addressing (at least partially) one of the main criticisms of GANs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08768v2-abstract-full').style.display = 'none'; document.getElementById('1802.08768v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1802.08195">arXiv:1802.08195</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1802.08195">pdf</a>, <a href="https://arxiv.org/format/1802.08195">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neurons and Cognition">q-bio.NC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Examples that Fool both Computer Vision and Time-Limited Humans
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elsayed%2C+G+F">Gamaleldin F. Elsayed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shankar%2C+S">Shreya Shankar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+B">Brian Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alex Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sohl-Dickstein%2C+J">Jascha Sohl-Dickstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1802.08195v3-abstract-short" style="display: inline;">
        Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08195v3-abstract-full').style.display = 'inline'; document.getElementById('1802.08195v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1802.08195v3-abstract-full" style="display: none;">
        Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1802.08195v3-abstract-full').style.display = 'none'; document.getElementById('1802.08195v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 February, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2018.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.07736">arXiv:1801.07736</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.07736">pdf</a>, <a href="https://arxiv.org/format/1801.07736">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MaskGAN: Better Text Generation via Filling in the______
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fedus%2C+W">William Fedus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.07736v3-abstract-short" style="display: inline;">
        Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07736v3-abstract-full').style.display = 'inline'; document.getElementById('1801.07736v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.07736v3-abstract-full" style="display: none;">
        Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.07736v3-abstract-full').style.display = 'none'; document.getElementById('1801.07736v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, ICLR 2018</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1801.02774">arXiv:1801.02774</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1801.02774">pdf</a>, <a href="https://arxiv.org/format/1801.02774">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Spheres
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gilmer%2C+J">Justin Gilmer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metz%2C+L">Luke Metz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faghri%2C+F">Fartash Faghri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schoenholz%2C+S+S">Samuel S. Schoenholz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raghu%2C+M">Maithra Raghu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wattenberg%2C+M">Martin Wattenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1801.02774v3-abstract-short" style="display: inline;">
        State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.02774v3-abstract-full').style.display = 'inline'; document.getElementById('1801.02774v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1801.02774v3-abstract-full" style="display: none;">
        State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size $O(1/\sqrt{d})$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1801.02774v3-abstract-full').style.display = 'none'; document.getElementById('1801.02774v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 September, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 January, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2018.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T45
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1710.08446">arXiv:1710.08446</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1710.08446">pdf</a>, <a href="https://arxiv.org/format/1710.08446">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Many Paths to Equilibrium: GANs Do Not Need to Decrease a Divergence At Every Step
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fedus%2C+W">William Fedus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosca%2C+M">Mihaela Rosca</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lakshminarayanan%2C+B">Balaji Lakshminarayanan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohamed%2C+S">Shakir Mohamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1710.08446v3-abstract-short" style="display: inline;">
        Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each playe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.08446v3-abstract-full').style.display = 'inline'; document.getElementById('1710.08446v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1710.08446v3-abstract-full" style="display: none;">
        Generative adversarial networks (GANs) are a family of generative models that do not minimize a single training criterion. Unlike other generative models, the data distribution is learned via a game between a generator (the generative model) and a discriminator (a teacher providing training signal) that each minimize their own cost. GANs are designed to reach a Nash equilibrium at which each player cannot reduce their cost without changing the other players&#39; parameters. One useful approach for the theory of GANs is to show that a divergence between the training distribution and the model distribution obtains its minimum value at equilibrium. Several recent research directions have been motivated by the idea that this divergence is the primary guide for the learning process and that every step of learning should decrease the divergence. We show that this view is overly restrictive. During GAN training, the discriminator provides learning signal in situations where the gradients of the divergences between distributions would not be useful. We provide empirical counterexamples to the view of GAN training as divergence minimization. Specifically, we demonstrate that GANs are able to learn distributions in situations where the divergence minimization point of view predicts they would fail. We also show that gradient penalties motivated from the divergence minimization perspective are equally helpful when applied in other contexts in which the divergence minimization perspective does not predict they would be helpful. This contributes to a growing body of evidence that GAN training may be more usefully viewed as approaching Nash equilibria via trajectories that do not necessarily minimize a specific divergence at each step.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.08446v3-abstract-full').style.display = 'none'; document.getElementById('1710.08446v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 February, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 October, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1708.08022">arXiv:1708.08022</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1708.08022">pdf</a>, <a href="https://arxiv.org/ps/1708.08022">ps</a>, <a href="https://arxiv.org/format/1708.08022">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/CSF.2017.10">10.1109/CSF.2017.10 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Protection of Private Information in Machine Learning Systems: Two Recent Approaches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erlingsson%2C+%C3%9A">lfar Erlingsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mironov%2C+I">Ilya Mironov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1708.08022v1-abstract-short" style="display: inline;">
        The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08022v1-abstract-full').style.display = 'inline'; document.getElementById('1708.08022v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1708.08022v1-abstract-full" style="display: none;">
        The recent, remarkable growth of machine learning has led to intense interest in the privacy of the data on which machine learning relies, and to new techniques for preserving privacy. However, older ideas about privacy may well remain valid and useful. This note reviews two recent works on privacy in the light of the wisdom of some of the early literature, in particular the principles distilled by Saltzer and Schroeder in the 1970s.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08022v1-abstract-full').style.display = 'none'; document.getElementById('1708.08022v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2017.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        IEEE 30th Computer Security Foundations Symposium (CSF), pages 1--6, 2017
      </p>
    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1705.07204">arXiv:1705.07204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1705.07204">pdf</a>, <a href="https://arxiv.org/format/1705.07204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ensemble Adversarial Training: Attacks and Defenses
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tram%C3%A8r%2C+F">Florian Tramr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boneh%2C+D">Dan Boneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1705.07204v4-abstract-short" style="display: inline;">
        Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07204v4-abstract-full').style.display = 'inline'; document.getElementById('1705.07204v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1705.07204v4-abstract-full" style="display: none;">
        Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model&#39;s loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.07204v4-abstract-full').style.display = 'none'; document.getElementById('1705.07204v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 May, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 5 figures, International Conference on Learning Representations (ICLR) 2018</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1704.03453">arXiv:1704.03453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1704.03453">pdf</a>, <a href="https://arxiv.org/format/1704.03453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Space of Transferable Adversarial Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tram%C3%A8r%2C+F">Florian Tramr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boneh%2C+D">Dan Boneh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1704.03453v2-abstract-short" style="display: inline;">
        Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.03453v2-abstract-full').style.display = 'inline'; document.getElementById('1704.03453v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1704.03453v2-abstract-full" style="display: none;">
        Adversarial examples are maliciously perturbed inputs designed to mislead machine learning (ML) models at test-time. They often transfer: the same adversarial example fools more than one model.
  In this work, we propose novel methods for estimating the previously unknown dimensionality of the space of adversarial inputs. We find that adversarial examples span a contiguous subspace of large (~25) dimensionality. Adversarial subspaces with higher dimensionality are more likely to intersect. We find that for two different models, a significant fraction of their subspaces is shared, thus enabling transferability.
  In the first quantitative analysis of the similarity of different models&#39; decision boundaries, we show that these boundaries are actually close in arbitrary directions, whether adversarial or benign. We conclude by formally studying the limits of transferability. We derive (1) sufficient conditions on the data distribution that imply transferability for simple model classes and (2) examples of scenarios in which transfer does not occur. These findings indicate that it may be possible to design defenses against transfer-based attacks, even for models that are vulnerable to direct attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.03453v2-abstract-full').style.display = 'none'; document.getElementById('1704.03453v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 7 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1702.02284">arXiv:1702.02284</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1702.02284">pdf</a>, <a href="https://arxiv.org/format/1702.02284">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Attacks on Neural Network Policies
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sandy Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+Y">Yan Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abbeel%2C+P">Pieter Abbeel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1702.02284v1-abstract-short" style="display: inline;">
        Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adver&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02284v1-abstract-full').style.display = 'inline'; document.getElementById('1702.02284v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1702.02284v1-abstract-full" style="display: none;">
        Machine learning classifiers are known to be vulnerable to inputs maliciously constructed by adversaries to force misclassification. Such adversarial examples have been extensively studied in the context of computer vision applications. In this work, we show adversarial attacks are also effective when targeting neural network policies in reinforcement learning. Specifically, we show existing adversarial example crafting techniques can be used to significantly degrade test-time performance of trained policies. Our threat model considers adversaries capable of introducing small perturbations to the raw input of the policy. We characterize the degree of vulnerability across tasks and training algorithms, for a subclass of adversarial-example attacks in white-box and black-box settings. Regardless of the learned task or training algorithm, we observe a significant drop in performance, even with small adversarial perturbations that do not interfere with human perception. Videos are available at http://rll.berkeley.edu/adversarial.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1702.02284v1-abstract-full').style.display = 'none'; document.getElementById('1702.02284v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2017.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1701.00160">arXiv:1701.00160</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1701.00160">pdf</a>, <a href="https://arxiv.org/format/1701.00160">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NIPS 2016 Tutorial: Generative Adversarial Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1701.00160v4-abstract-short" style="display: inline;">
        This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs wi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00160v4-abstract-full').style.display = 'inline'; document.getElementById('1701.00160v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1701.00160v4-abstract-full" style="display: none;">
        This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1701.00160v4-abstract-full').style.display = 'none'; document.getElementById('1701.00160v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 December, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">v2-v4 are all typo fixes. No substantive changes relative to v1</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1611.01236">arXiv:1611.01236</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1611.01236">pdf</a>, <a href="https://arxiv.org/format/1611.01236">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Machine Learning at Scale
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1611.01236v2-abstract-short" style="display: inline;">
        Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01236v2-abstract-full').style.display = 'inline'; document.getElementById('1611.01236v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1611.01236v2-abstract-full" style="display: none;">
        Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model&#39;s parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a &#34;label leaking&#34; effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1611.01236v2-abstract-full').style.display = 'none'; document.getElementById('1611.01236v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 November, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 5 figures</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1610.05755">arXiv:1610.05755</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1610.05755">pdf</a>, <a href="https://arxiv.org/format/1610.05755">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erlingsson%2C+%C3%9A">lfar Erlingsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1610.05755v4-abstract-short" style="display: inline;">
        Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.05755v4-abstract-full').style.display = 'inline'; document.getElementById('1610.05755v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1610.05755v4-abstract-full" style="display: none;">
        Some machine learning applications involve training data that is sensitive, such as the medical histories of patients in a clinical trial. A model may inadvertently and implicitly store some of its training data; careful analysis of the model may therefore reveal sensitive information.
  To address this problem, we demonstrate a generally applicable approach to providing strong privacy guarantees for training data: Private Aggregation of Teacher Ensembles (PATE). The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as &#34;teachers&#34; for a &#34;student&#34; model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student&#39;s privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student&#39;s training) and formally, in terms of differential privacy. These properties hold even if an adversary can not only query the student but also inspect its internal workings.
  Compared with previous work, the approach imposes only weak assumptions on how teachers are trained: it applies to any model, including non-convex models like DNNs. We achieve state-of-the-art privacy/utility trade-offs on MNIST and SVHN thanks to an improved privacy analysis and semi-supervised learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.05755v4-abstract-full').style.display = 'none'; document.getElementById('1610.05755v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 October, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICLR 17 as an oral</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1610.00768">arXiv:1610.00768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1610.00768">pdf</a>, <a href="https://arxiv.org/ps/1610.00768">ps</a>, <a href="https://arxiv.org/format/1610.00768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Technical Report on the CleverHans v2.1.0 Adversarial Examples Library
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faghri%2C+F">Fartash Faghri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlini%2C+N">Nicholas Carlini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feinman%2C+R">Reuben Feinman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Cihang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+Y">Yash Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+T">Tom Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roy%2C+A">Aurko Roy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matyasko%2C+A">Alexander Matyasko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behzadan%2C+V">Vahid Behzadan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hambardzumyan%2C+K">Karen Hambardzumyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhishuai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Juang%2C+Y">Yi-Lin Juang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sheatsley%2C+R">Ryan Sheatsley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garg%2C+A">Abhibhav Garg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Uesato%2C+J">Jonathan Uesato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gierke%2C+W">Willi Gierke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yinpeng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berthelot%2C+D">David Berthelot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hendricks%2C+P">Paul Hendricks</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rauber%2C+J">Jonas Rauber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+R">Rujun Long</a>
      , et al. (1 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1610.00768v6-abstract-short" style="display: inline;">
        CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial exam&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.00768v6-abstract-full').style.display = 'inline'; document.getElementById('1610.00768v6-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1610.00768v6-abstract-full" style="display: none;">
        CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models&#39; performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure.
  This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the CleverHans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1610.00768v6-abstract-full').style.display = 'none'; document.getElementById('1610.00768v6-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 October, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical report for https://github.com/tensorflow/cleverhans</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.02533">arXiv:1607.02533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.02533">pdf</a>, <a href="https://arxiv.org/format/1607.02533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial examples in the physical world
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kurakin%2C+A">Alexey Kurakin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+S">Samy Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.02533v4-abstract-short" style="display: inline;">
        Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.02533v4-abstract-full').style.display = 'inline'; document.getElementById('1607.02533v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.02533v4-abstract-full" style="display: none;">
        Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.02533v4-abstract-full').style.display = 'none'; document.getElementById('1607.02533v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 February, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, 6 figures. Demo available at https://youtu.be/zQ_uMenoBCk</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1607.00133">arXiv:1607.00133</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1607.00133">pdf</a>, <a href="https://arxiv.org/format/1607.00133">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/2976749.2978318">10.1145/2976749.2978318 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning with Differential Privacy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+A">Andy Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McMahan%2C+H+B">H. Brendan McMahan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mironov%2C+I">Ilya Mironov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Li Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1607.00133v2-abstract-short" style="display: inline;">
        Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00133v2-abstract-full').style.display = 'inline'; document.getElementById('1607.00133v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1607.00133v2-abstract-full" style="display: none;">
        Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1607.00133v2-abstract-full').style.display = 'none'; document.getElementById('1607.00133v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 October, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2016.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (ACM CCS), pp. 308-318, 2016
      </p>
    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1606.03498">arXiv:1606.03498</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1606.03498">pdf</a>, <a href="https://arxiv.org/format/1606.03498">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improved Techniques for Training GANs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Salimans%2C+T">Tim Salimans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaremba%2C+W">Wojciech Zaremba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+V">Vicki Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Radford%2C+A">Alec Radford</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xi Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1606.03498v1-abstract-short" style="display: inline;">
        We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, n&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.03498v1-abstract-full').style.display = 'inline'; document.getElementById('1606.03498v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1606.03498v1-abstract-full" style="display: none;">
        We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1606.03498v1-abstract-full').style.display = 'none'; document.getElementById('1606.03498v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 June, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2016.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07725">arXiv:1605.07725</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07725">pdf</a>, <a href="https://arxiv.org/ps/1605.07725">ps</a>, <a href="https://arxiv.org/format/1605.07725">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Training Methods for Semi-Supervised Text Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Miyato%2C+T">Takeru Miyato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+A+M">Andrew M. Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07725v3-abstract-short" style="display: inline;">
        Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07725v3-abstract-full').style.display = 'inline'; document.getElementById('1605.07725v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07725v3-abstract-full" style="display: none;">
        Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overfitting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07725v3-abstract-full').style.display = 'none'; document.getElementById('1605.07725v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 May, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2017</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07277">arXiv:1605.07277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07277">pdf</a>, <a href="https://arxiv.org/format/1605.07277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07277v1-abstract-short" style="display: inline;">
        Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07277v1-abstract-full').style.display = 'inline'; document.getElementById('1605.07277v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07277v1-abstract-full" style="display: none;">
        Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07277v1-abstract-full').style.display = 'none'; document.getElementById('1605.07277v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1605.07157">arXiv:1605.07157</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1605.07157">pdf</a>, <a href="https://arxiv.org/format/1605.07157">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Learning for Physical Interaction through Video Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Finn%2C+C">Chelsea Finn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levine%2C+S">Sergey Levine</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1605.07157v4-abstract-short" style="display: inline;">
        A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about ph&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07157v4-abstract-full').style.display = 'inline'; document.getElementById('1605.07157v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1605.07157v4-abstract-full" style="display: none;">
        A core challenge for an agent learning to interact with the world is to predict how its actions affect objects in its environment. Many existing methods for learning the dynamics of physical interactions require labeled object information. However, to scale real-world interaction learning to a variety of scenes and objects, acquiring labeled data becomes increasingly impractical. To learn about physical object motion without labels, we develop an action-conditioned video prediction model that explicitly models pixel motion, by predicting a distribution over pixel motion from previous frames. Because our model explicitly predicts motion, it is partially invariant to object appearance, enabling it to generalize to previously unseen objects. To explore video prediction for real-world interactive agents, we also introduce a dataset of 59,000 robot interactions involving pushing motions, including a test set with novel objects. In this dataset, accurate prediction of videos conditioned on the robot&#39;s future actions amounts to learning a &#34;visual imagination&#34; of different futures based on different courses of action. Our experiments show that our proposed method produces more accurate video predictions both quantitatively and qualitatively, when compared to prior methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1605.07157v4-abstract-full').style.display = 'none'; document.getElementById('1605.07157v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 October, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 May, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in NIPS &#39;16; Video results, code, and data available at: http://www.sites.google.com/site/robotprediction</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1604.04326">arXiv:1604.04326</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1604.04326">pdf</a>, <a href="https://arxiv.org/format/1604.04326">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving the Robustness of Deep Neural Networks via Stability Training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+S">Stephan Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yang Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leung%2C+T">Thomas Leung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1604.04326v1-abstract-short" style="display: inline;">
        In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep network&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.04326v1-abstract-full').style.display = 'inline'; document.getElementById('1604.04326v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1604.04326v1-abstract-full" style="display: none;">
        In this paper we address the issue of output instability of deep neural networks: small perturbations in the visual input can significantly distort the feature embeddings and output of a neural network. Such instability affects many deep architectures with state-of-the-art performance on a wide range of computer vision tasks. We present a general stability training method to stabilize deep networks against small input distortions that result from various types of common image processing, such as compression, rescaling, and cropping. We validate our method by stabilizing the state-of-the-art Inception architecture against these types of distortions. In addition, we demonstrate that our stabilized model gives robust state-of-the-art performance on large-scale near-duplicate detection, similar-image ranking, and classification on noisy datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.04326v1-abstract-full').style.display = 'none'; document.getElementById('1604.04326v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in CVPR 2016</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1603.04467">arXiv:1603.04467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1603.04467">pdf</a>, <a href="https://arxiv.org/format/1603.04467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abadi%2C+M">Martn Abadi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+A">Ashish Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barham%2C+P">Paul Barham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brevdo%2C+E">Eugene Brevdo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhifeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Citro%2C+C">Craig Citro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Corrado%2C+G+S">Greg S. Corrado</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Davis%2C+A">Andy Davis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dean%2C+J">Jeffrey Dean</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Devin%2C+M">Matthieu Devin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghemawat%2C+S">Sanjay Ghemawat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harp%2C+A">Andrew Harp</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Irving%2C+G">Geoffrey Irving</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Isard%2C+M">Michael Isard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+Y">Yangqing Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jozefowicz%2C+R">Rafal Jozefowicz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaiser%2C+L">Lukasz Kaiser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kudlur%2C+M">Manjunath Kudlur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levenberg%2C+J">Josh Levenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mane%2C+D">Dan Mane</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Monga%2C+R">Rajat Monga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moore%2C+S">Sherry Moore</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Murray%2C+D">Derek Murray</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Olah%2C+C">Chris Olah</a>
      , et al. (15 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1603.04467v2-abstract-short" style="display: inline;">
        TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational de&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1603.04467v2-abstract-full').style.display = 'inline'; document.getElementById('1603.04467v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1603.04467v2-abstract-full" style="display: none;">
        TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1603.04467v2-abstract-full').style.display = 'none'; document.getElementById('1603.04467v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 March, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Version 2 updates only the metadata, to correct the formatting of Martn Abadi&#39;s name</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1602.02697">arXiv:1602.02697</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1602.02697">pdf</a>, <a href="https://arxiv.org/format/1602.02697">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Practical Black-Box Attacks against Machine Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Papernot%2C+N">Nicolas Papernot</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McDaniel%2C+P">Patrick McDaniel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jha%2C+S">Somesh Jha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Celik%2C+Z+B">Z. Berkay Celik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Swami%2C+A">Ananthram Swami</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1602.02697v4-abstract-short" style="display: inline;">
        Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.02697v4-abstract-full').style.display = 'inline'; document.getElementById('1602.02697v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1602.02697v4-abstract-full" style="display: none;">
        Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.02697v4-abstract-full').style.display = 'none'; document.getElementById('1602.02697v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 February, 2016;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.05644">arXiv:1511.05644</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.05644">pdf</a>, <a href="https://arxiv.org/format/1511.05644">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adversarial Autoencoders
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Makhzani%2C+A">Alireza Makhzani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaitly%2C+N">Navdeep Jaitly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frey%2C+B">Brendan Frey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.05644v2-abstract-short" style="display: inline;">
        In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05644v2-abstract-full').style.display = 'inline'; document.getElementById('1511.05644v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.05644v2-abstract-full" style="display: none;">
        In this paper, we propose the &#34;adversarial autoencoder&#34; (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05644v2-abstract-full').style.display = 'none'; document.getElementById('1511.05644v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1511.05641">arXiv:1511.05641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1511.05641">pdf</a>, <a href="https://arxiv.org/format/1511.05641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Net2Net: Accelerating Learning via Knowledge Transfer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tianqi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shlens%2C+J">Jonathon Shlens</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1511.05641v4-abstract-short" style="display: inline;">
        We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05641v4-abstract-full').style.display = 'inline'; document.getElementById('1511.05641v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1511.05641v4-abstract-full" style="display: none;">
        We introduce techniques for rapidly transferring the information stored in one neural net into another neural net. The main purpose is to accelerate the training of a significantly larger neural net. During real-world workflows, one often trains very many different neural networks during the experimentation and design process. This is a wasteful process in which each new model is trained from scratch. Our Net2Net technique accelerates the experimentation process by instantaneously transferring the knowledge from a previous network to each new deeper or wider network. Our techniques are based on the concept of function-preserving transformations between neural network specifications. This differs from previous approaches to pre-training that altered the function represented by a neural net when adding layers to it. Using our knowledge transfer mechanism to add depth to Inception modules, we demonstrate a new state of the art accuracy rating on the ImageNet dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1511.05641v4-abstract-full').style.display = 'none'; document.getElementById('1511.05641v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2016; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 November, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2016 submission</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1510.01799">arXiv:1510.01799</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1510.01799">pdf</a>, <a href="https://arxiv.org/ps/1510.01799">ps</a>, <a href="https://arxiv.org/format/1510.01799">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Per-Example Gradient Computations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1510.01799v2-abstract-short" style="display: inline;">
        This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1510.01799v2-abstract-full" style="display: none;">
        This technical report describes an efficient technique for computing the norm of the gradient of the loss function for a neural network with respect to its parameters. This gradient norm can be computed efficiently for every example.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1510.01799v2-abstract-full').style.display = 'none'; document.getElementById('1510.01799v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 October, 2015; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 October, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This revision fixed some typos. Many thanks to Hugo Larochelle for reporting them!</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1312.6199">arXiv:1312.6199</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1312.6199">pdf</a>, <a href="https://arxiv.org/format/1312.6199">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intriguing properties of neural networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Szegedy%2C+C">Christian Szegedy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zaremba%2C+W">Wojciech Zaremba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sutskever%2C+I">Ilya Sutskever</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bruna%2C+J">Joan Bruna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erhan%2C+D">Dumitru Erhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fergus%2C+R">Rob Fergus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1312.6199v4-abstract-short" style="display: inline;">
        Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction betwee&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1312.6199v4-abstract-full').style.display = 'inline'; document.getElementById('1312.6199v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1312.6199v4-abstract-full" style="display: none;">
        Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties.
  First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks.
  Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network&#39;s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1312.6199v4-abstract-full').style.display = 'none'; document.getElementById('1312.6199v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2014; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 December, 2013;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2013.
      
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1212.2686">arXiv:1212.2686</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1212.2686">pdf</a>, <a href="https://arxiv.org/ps/1212.2686">ps</a>, <a href="https://arxiv.org/format/1212.2686">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Training of Deep Boltzmann Machines
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1212.2686v1-abstract-short" style="display: inline;">
        We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1212.2686v1-abstract-full" style="display: none;">
        We introduce a new method for training deep Boltzmann machines jointly. Prior methods require an initial learning pass that trains the deep Boltzmann machine greedily, one layer at a time, or do not perform well on classifi- cation tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1212.2686v1-abstract-full').style.display = 'none'; document.getElementById('1212.2686v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 December, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">4 pages</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1211.5590">arXiv:1211.5590</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1211.5590">pdf</a>, <a href="https://arxiv.org/format/1211.5590">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Symbolic Computation">cs.SC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Theano: new features and speed improvements
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bastien%2C+F">Frdric Bastien</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lamblin%2C+P">Pascal Lamblin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pascanu%2C+R">Razvan Pascanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergstra%2C+J">James Bergstra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bergeron%2C+A">Arnaud Bergeron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bouchard%2C+N">Nicolas Bouchard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Warde-Farley%2C+D">David Warde-Farley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1211.5590v1-abstract-short" style="display: inline;">
        Theano is a linear algebra compiler that optimizes a user's symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano's performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1211.5590v1-abstract-full').style.display = 'inline'; document.getElementById('1211.5590v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1211.5590v1-abstract-full" style="display: none;">
        Theano is a linear algebra compiler that optimizes a user&#39;s symbolically-specified mathematical computations to produce efficient low-level implementations. In this paper, we present new features and efficiency improvements to Theano, and benchmarks demonstrating Theano&#39;s performance relative to Torch7, a recently introduced machine learning library, and to RNNLM, a C++ library targeted at recurrent neural networks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1211.5590v1-abstract-full').style.display = 'none'; document.getElementById('1211.5590v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 November, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at the Deep Learning Workshop, NIPS 2012</span>
    </p>
    

    

    
  </li>"arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1206.6407">arXiv:1206.6407</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1206.6407">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large-Scale Feature Learning With Spike-and-Slab Sparse Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goodfellow%2C+I">Ian Goodfellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1206.6407v1-abstract-short" style="display: inline;">
        We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enorm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1206.6407v1-abstract-full').style.display = 'inline'; document.getElementById('1206.6407v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1206.6407v1-abstract-full" style="display: none;">
        We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models? Transfer Learning Challenge.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1206.6407v1-abstract-full').style.display = 'none'; document.getElementById('1206.6407v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Appears in Proceedings of the 29th International Conference on Machine Learning (ICML 2012). arXiv admin note: substantial text overlap with arXiv:1201.3382</span>
    </p>
    

    

    
  </li>